# GCP NVIDIA L4 Validation Report - v0.4.6

**Platform**: GCP Compute Engine
**Instance**: g2-standard-4
**GPU**: NVIDIA L4 (24GB)
**Date**: 2026-01-19
**PyTorch Version**: 2.7.1+cu128
**CUDA Compute**: 8.9

---

## Test Results Summary

| Test Suite | Passed | Total | Status |
|------------|--------|-------|--------|
| NVIDIA Backend | 66 | 66 | ✅ PASS |
| MoE Tests | 48 | 48 | ✅ PASS |
| FP8 Native | 51 | 51 | ✅ PASS |
| FP8 Training | 20 | 20 | ✅ PASS |
| **Total** | **185** | **185** | **✅ 100%** |

---

## Demo Results Summary

| Demo | Status | Details |
|------|--------|---------|
| MoE Demo | ✅ PASS | 8/8 demonstrations |
| FP8 Native Demo | ✅ PASS | 8/8 demonstrations |
| FlexAttention Demo | ✅ PASS | 9/9 demonstrations |
| **Total** | **✅ PASS** | **25/25 demonstrations** |

---

## Performance Benchmarks

### MoE Throughput (8 experts, top-2)

| Hidden Size | Batch | Seq Len | Time/Batch | Throughput |
|-------------|-------|---------|------------|------------|
| 512 | 32 | 128 | 7.79ms | 525.7K tok/s |
| 1024 | 32 | 128 | 13.17ms | 311.0K tok/s |

### FP8 Performance

| Operation | Status | Details |
|-----------|--------|---------|
| E4M3 Quantization | ✅ Available | Native PyTorch 2.1+ |
| E5M2 Quantization | ✅ Available | Native PyTorch 2.1+ |
| Hardware FP8 | ⚠️ Limited | Compute 8.9 (H100 required for full support) |

---

## Bug Fixes Applied During Validation

The following device compatibility issues were identified and fixed during GCP testing:

1. **MoE Router Device Mismatch** (`routing.py`)
   - `noise_std` buffer device fix
   - `expert_utilization_ema` buffer device fix
   - `capacity_factors` buffer device fix
   - Device propagation to `token_projection` (HashRouter)
   - Device propagation to `complexity_analyzer` (DynamicCapacityRouter)

2. **MoE Layer Device Mismatch** (`moe_layers.py`)
   - `expert_usage_count` buffer device fix
   - `input_complexity_ema` buffer device fix

3. **FP8 Test Device Handling** (`test_fp8_native.py`, `test_fp8_training.py`)
   - Device type comparison fix (cuda vs cuda:0)
   - FP8 isfinite conversion fix
   - Device fixture additions

---

## Hardware Information

```
GPU: NVIDIA L4
Memory: 23GB
Compute Capability: 8.9
CUDA: 12.8
PyTorch: 2.7.1+cu128
```

---

## Files in This Report

- `nvidia_backend.txt` - NVIDIA backend test output
- `moe_tests.txt` - MoE test output
- `fp8_tests.txt` - FP8 test output
- `moe_demo.txt` - MoE demo output
- `fp8_demo.txt` - FP8 demo output
- `flex_attention_demo.txt` - FlexAttention demo output
- `moe_benchmark.txt` - MoE performance benchmark

---

## Conclusion

v0.4.6 validation on GCP NVIDIA L4 is **COMPLETE** with all 185 tests and 25 demonstrations passing. Performance benchmarks show expected throughput for the L4 GPU class.

Key findings:
- All MoE features (5 layer types, 5 routing strategies) working correctly on GPU
- Native FP8 quantization available (E4M3/E5M2)
- FlexAttention patterns all functional
- Device compatibility issues identified and fixed

---

*Report generated by KernelPyTorch v0.4.6 Cloud Validation*
