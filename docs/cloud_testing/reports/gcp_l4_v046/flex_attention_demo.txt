/home/shahrahman/kernel_pytorch/attention/distributed/ring_attention.py:41: UserWarning: Hardware Abstraction Layer not available - using basic device coordination
  warnings.warn("Hardware Abstraction Layer not available - using basic device coordination")
/home/shahrahman/kernel_pytorch/attention/distributed/context_parallel.py:41: UserWarning: Hardware Abstraction Layer not available - using basic device coordination
  warnings.warn("Hardware Abstraction Layer not available - using basic device coordination")
/home/shahrahman/kernel_pytorch/precision/fp8_training_engine.py:34: UserWarning: Transformer Engine not available - FP8 training will use fallback implementations
  warnings.warn("Transformer Engine not available - FP8 training will use fallback implementations")
/home/shahrahman/kernel_pytorch/distributed_scale/__init__.py:28: FutureWarning: communication_optimization.py has been refactored into multiple focused modules. Consider importing from the specific modules directly: communication_primitives, network_optimization, communication_profiling
  from .communication_optimization import (
/home/shahrahman/kernel_pytorch/distributed_scale/__init__.py:35: FutureWarning: hardware_adaptation.py has been refactored into multiple focused modules. Consider importing from the specific modules directly: hardware_discovery, thermal_power_management, fault_tolerance, hardware_adapter
  from .hardware_adaptation import (
/home/shahrahman/kernel_pytorch/distributed_scale/__init__.py:42: FutureWarning: orchestration.py has been refactored into multiple focused modules. Consider importing from the specific modules directly: job_management, cluster_management, scaling_fault_tolerance
  from .orchestration import (
/home/shahrahman/kernel_pytorch/hardware/__init__.py:28: UserWarning: Hardware abstraction import failed: No module named 'kernel_pytorch.hardware.distributed_scale'
  warnings.warn(f"Hardware abstraction import failed: {e}")
/home/shahrahman/kernel_pytorch/precision/fp8_optimizations.py:32: UserWarning: Transformer Engine not available - using fallback FP8 implementations
  warnings.warn("Transformer Engine not available - using fallback FP8 implementations")
/home/shahrahman/kernel_pytorch/attention/implementations/flex_attention.py:393: UserWarning: FlexAttention failed: indices should be either on cpu or on the same device as the indexed tensor (cpu). Falling back to standard attention.
  warnings.warn(f"FlexAttention failed: {e}. Falling back to standard attention.")

============================================================
  FlexAttention Demo - KernelPyTorch v0.4.4
============================================================

======================================================================
  FlexAttention Availability Check
======================================================================
PyTorch Version: 2.7.1+cu128
FlexAttention Available: True
torch.compile Available: True

Supported Patterns:
  - causal
  - sliding_window
  - causal_sliding_window
  - alibi
  - document_masking
  - prefix_lm
  - soft_cap
  - custom

======================================================================
  Basic FlexAttention
======================================================================
Device: cuda
Input shape: torch.Size([2, 128, 256])
Output shape: torch.Size([2, 128, 256])
Stats: {'pattern': 'full', 'embed_dim': 256, 'num_heads': 4, 'head_dim': 64, 'scale': 0.125, 'max_sequence_length': 8192, 'parameters': 262144, 'trainable_parameters': 262144, 'flex_attention_available': True, 'using_flex_attention': False, 'score_mod_compiled': True, 'using_block_mask': True, 'block_mask_cache_size': 0}

======================================================================
  Causal Attention Pattern
======================================================================
Pattern: Causal (autoregressive)
Input: torch.Size([2, 64, 256]) -> Output: torch.Size([2, 64, 256])
Use case: Language modeling, text generation

======================================================================
  Sliding Window Attention
======================================================================
Pattern: Sliding Window (size=32)
Input: torch.Size([2, 128, 256]) -> Output: torch.Size([2, 128, 256])
Use case: Long sequence processing with local context
Memory: O(N*W) vs O(N^2) for full attention

======================================================================
  Custom Score Modification
======================================================================
Custom score_mod: Soft capping (Gemma 2 style)
Input: torch.Size([2, 64, 256]) -> Output: torch.Size([2, 64, 256])
Use case: Improved training stability for large models

======================================================================
  ALiBi Attention
======================================================================
Pattern: ALiBi (Attention with Linear Biases)
Input: torch.Size([2, 128, 256]) -> Output: torch.Size([2, 128, 256])
Use case: Length extrapolation without positional embeddings

======================================================================
  Performance Comparison
======================================================================
Comparing FlexAttention vs FlashAttention-3
Device: cuda, Batch: 4, Embed: 512, Heads: 8

  Seq=  64: Flex=17.03ms, Standard=0.79ms, Ratio=0.05x
  Seq= 128: Flex=17.80ms, Standard=0.80ms, Ratio=0.05x
  Seq= 256: Flex=17.46ms, Standard=0.81ms, Ratio=0.05x
  Seq= 512: Flex=17.17ms, Standard=1.57ms, Ratio=0.09x

======================================================================
  Transformer Block with FlexAttention
======================================================================
Transformer block with FlexAttention causal pattern
Input: torch.Size([2, 64, 256]) -> Output: torch.Size([2, 64, 256])
Components: LayerNorm -> FlexAttention -> LayerNorm -> FFN
Gradient flow: OK (grad shape: torch.Size([2, 64, 256]))

======================================================================
  Registry Integration
======================================================================
Registered attention implementations:
    dynamic_sparse_attention
    flash_attention
    flash_attention2
    flash_attention3
    flashattention2
    flashattention3
  * flex_attention (FlexAttention)
  * flex_attention_causal (FlexAttention)
  * flex_attention_sliding_window (FlexAttention)
    memory_efficient_attention

Created via registry: FlexAttentionLayer

======================================================================
  Demo Summary
======================================================================
Results: 9/9 demos passed

  [PASS] availability
  [PASS] basic
  [PASS] causal
  [PASS] sliding_window
  [PASS] custom_score_mod
  [PASS] alibi
  [PASS] performance
  [PASS] transformer_block
  [PASS] registry
