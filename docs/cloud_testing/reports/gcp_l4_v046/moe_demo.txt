/home/shahrahman/kernel_pytorch/attention/distributed/ring_attention.py:41: UserWarning: Hardware Abstraction Layer not available - using basic device coordination
  warnings.warn("Hardware Abstraction Layer not available - using basic device coordination")
/home/shahrahman/kernel_pytorch/attention/distributed/context_parallel.py:41: UserWarning: Hardware Abstraction Layer not available - using basic device coordination
  warnings.warn("Hardware Abstraction Layer not available - using basic device coordination")
/home/shahrahman/kernel_pytorch/precision/fp8_training_engine.py:34: UserWarning: Transformer Engine not available - FP8 training will use fallback implementations
  warnings.warn("Transformer Engine not available - FP8 training will use fallback implementations")
/home/shahrahman/kernel_pytorch/distributed_scale/__init__.py:28: FutureWarning: communication_optimization.py has been refactored into multiple focused modules. Consider importing from the specific modules directly: communication_primitives, network_optimization, communication_profiling
  from .communication_optimization import (
/home/shahrahman/kernel_pytorch/distributed_scale/__init__.py:35: FutureWarning: hardware_adaptation.py has been refactored into multiple focused modules. Consider importing from the specific modules directly: hardware_discovery, thermal_power_management, fault_tolerance, hardware_adapter
  from .hardware_adaptation import (
/home/shahrahman/kernel_pytorch/distributed_scale/__init__.py:42: FutureWarning: orchestration.py has been refactored into multiple focused modules. Consider importing from the specific modules directly: job_management, cluster_management, scaling_fault_tolerance
  from .orchestration import (
/home/shahrahman/kernel_pytorch/hardware/__init__.py:28: UserWarning: Hardware abstraction import failed: No module named 'kernel_pytorch.hardware.distributed_scale'
  warnings.warn(f"Hardware abstraction import failed: {e}")
/home/shahrahman/kernel_pytorch/precision/fp8_optimizations.py:32: UserWarning: Transformer Engine not available - using fallback FP8 implementations
  warnings.warn("Transformer Engine not available - using fallback FP8 implementations")
============================================================
Mixture of Experts (MoE) Demonstrations
kernel_pytorch v0.4.6
============================================================

============================================================
Demo 1: Basic MoE Layer
============================================================
Config: 8 experts, top-2 routing
Created MoE layer with 8 experts
Input shape: torch.Size([4, 32, 256])
Output shape: torch.Size([4, 32, 256])

Using create_moe convenience function: 8 experts

============================================================
Demo 2: Different MoE Layer Types
============================================================
Standard MoE output: torch.Size([2, 16, 256])
Sparse MoE (25% sparsity) output: torch.Size([2, 16, 256])
Switch Transformer (top-1) output: torch.Size([2, 16, 256])
GLaM-style MoE output: torch.Size([2, 16, 256])
Adaptive MoE output: torch.Size([2, 16, 256])
  Factory 'standard': MoELayer
  Factory 'sparse': SparseMoELayer
  Factory 'switch': SwitchTransformerMoE
  Factory 'glam': GLaMStyleMoE
  Factory 'adaptive': AdaptiveMoELayer

============================================================
Demo 3: Routing Strategies
============================================================
TopK Router:
  Expert indices shape: torch.Size([100, 2])
  Expert weights shape: torch.Size([100, 2])
Switch Router (top-1):
  Expert indices shape: torch.Size([100, 1])
Hash Router:
  Deterministic expert assignment: [[2, 4], [2, 4], [1, 7], [2, 6], [4, 1]]
Learned Router:
  Router features shape: torch.Size([100, 128])
Dynamic Capacity Router:
  Complexity scores shape: torch.Size([100])
  Factory 'topk': TopKRouter
  Factory 'switch': SwitchRouter
  Factory 'hash': HashRouter
  Factory 'learned': LearnedRouter
  Factory 'dynamic': DynamicCapacityRouter

============================================================
Demo 4: Expert Network Architectures
============================================================
FeedForward Expert: 525,568 params, output shape: torch.Size([100, 256])
Convolutional Expert: 6,819,072 params, output shape: torch.Size([100, 256])
Attention Expert: 4,726,016 params, output shape: torch.Size([100, 256])
Parameter-Efficient Expert: 567,552 params, output shape: torch.Size([100, 256])

============================================================
Demo 5: Load Balancing
============================================================
Expert capacities: [157, 157, 157, 157, 157, 157, 157, 157]
Tokens dropped: 0
Load balance loss (switch): 2.0017
Assignment entropy: 2.079
Max/Min assignment ratio: 1.06/0.91

MoE with auxiliary losses:
  load_balance_loss: 0.020081
  router_z_loss: 0.004585
  tokens_dropped: 0.000000

============================================================
Demo 6: Training MoE Models
============================================================
Training for 5 steps...
  Step 1: main_loss=2.3073, aux_loss=7.024472
  Step 2: main_loss=2.3053, aux_loss=1.024472
  Step 3: main_loss=2.3204, aux_loss=0.024492
  Step 4: main_loss=2.2924, aux_loss=0.024500
  Step 5: main_loss=2.3111, aux_loss=0.024426

Expert utilization after training:
  Balance: 0.998
  Efficiency: 0.250

============================================================
Demo 7: MoE in Transformer
============================================================
MoE Transformer:
  Input: torch.Size([2, 32])
  Output logits: torch.Size([2, 32, 1000])
  Auxiliary losses: 8 items
  Total params: 9,585,640
  MoE params: 8,413,184 (87.8%)

============================================================
Demo 8: Performance Comparison
============================================================
Input shape: (8, 128, 512)

Standard FFN:
  Parameters: 2,099,712
  Time: 30.40 ms

MoE (8 experts, top-2):
  Parameters: 16,801,792
  Time: 105.46 ms
  Parameter ratio: 8.0x
  Time ratio: 3.5x

============================================================
All demos completed!
============================================================
