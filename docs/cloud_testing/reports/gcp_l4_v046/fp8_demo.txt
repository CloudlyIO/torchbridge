/home/shahrahman/kernel_pytorch/attention/distributed/ring_attention.py:41: UserWarning: Hardware Abstraction Layer not available - using basic device coordination
  warnings.warn("Hardware Abstraction Layer not available - using basic device coordination")
/home/shahrahman/kernel_pytorch/attention/distributed/context_parallel.py:41: UserWarning: Hardware Abstraction Layer not available - using basic device coordination
  warnings.warn("Hardware Abstraction Layer not available - using basic device coordination")
/home/shahrahman/kernel_pytorch/precision/fp8_training_engine.py:34: UserWarning: Transformer Engine not available - FP8 training will use fallback implementations
  warnings.warn("Transformer Engine not available - FP8 training will use fallback implementations")
/home/shahrahman/kernel_pytorch/distributed_scale/__init__.py:28: FutureWarning: communication_optimization.py has been refactored into multiple focused modules. Consider importing from the specific modules directly: communication_primitives, network_optimization, communication_profiling
  from .communication_optimization import (
/home/shahrahman/kernel_pytorch/distributed_scale/__init__.py:35: FutureWarning: hardware_adaptation.py has been refactored into multiple focused modules. Consider importing from the specific modules directly: hardware_discovery, thermal_power_management, fault_tolerance, hardware_adapter
  from .hardware_adaptation import (
/home/shahrahman/kernel_pytorch/distributed_scale/__init__.py:42: FutureWarning: orchestration.py has been refactored into multiple focused modules. Consider importing from the specific modules directly: job_management, cluster_management, scaling_fault_tolerance
  from .orchestration import (
/home/shahrahman/kernel_pytorch/hardware/__init__.py:28: UserWarning: Hardware abstraction import failed: No module named 'kernel_pytorch.hardware.distributed_scale'
  warnings.warn(f"Hardware abstraction import failed: {e}")
/home/shahrahman/kernel_pytorch/precision/fp8_optimizations.py:32: UserWarning: Transformer Engine not available - using fallback FP8 implementations
  warnings.warn("Transformer Engine not available - using fallback FP8 implementations")

============================================================
  Native FP8 Demo - KernelPyTorch v0.4.5
  Full FP8 with Native PyTorch Types
============================================================

======================================================================
  FP8 Availability Check
======================================================================
PyTorch Version: 2.7.1+cu128
Native FP8 Available: True
FP8 scaled_mm Available: True

Supported FP8 Formats:
  - e4m3fn
  - e5m2

FP8 Format Specifications:
  E4M3FN max value: 448.0
  E5M2 max value: 57344.0

Recommended Usage:
  e4m3fn: Forward pass (weights, activations) - higher precision
  e5m2: Backward pass (gradients) - wider dynamic range

======================================================================
  FP8 Quantization
======================================================================
Device: cuda

Original tensor shape: torch.Size([4, 256])
Original range: [-3.4118, 3.4024]

E4M3 Quantization:
  Scale: 131.3109
  MSE: 0.000703
  Relative error: 2.6808%

E5M2 Quantization:
  Scale: 16807.7969
  MSE: 0.002734
  Relative error: 5.2863%

Conclusion: E4M3 provides 3.9x lower error than E5M2

======================================================================
  Native FP8 Linear Layer
======================================================================
Device: cuda

Layer configuration:
  NativeFP8Linear(in_features=512, out_features=256, bias=True, weight_format=e4m3fn, fp8_native=True)

FP8 Layer Info:
  Weight format: e4m3fn
  Activation format: e4m3fn
  Weight scale: 5068.6533
  FP8 native: True
  FP8 active: True

Forward pass:
  Input shape: torch.Size([4, 512])
  Output shape: torch.Size([4, 256])
  Output finite: True
  Gradient flow: OK (grad shape: torch.Size([4, 512]))

======================================================================
  FP8 Inference Engine
======================================================================
Device: cuda

Original model: 6 Linear layers

Memory Analysis:
  FP32 memory: 3.00 MB
  FP8 memory: 0.75 MB
  Savings: 75.0%
  FP8 layers: 6

Inference:
  Input shape: torch.Size([2, 32, 256])
  Output shape: torch.Size([2, 32, 256])
  Output finite: True

======================================================================
  Model Conversion to Native FP8
======================================================================
Converting model to native FP8...
Converted 6 Linear layers to NativeFP8Linear
Converted 6 layers to NativeFP8Linear

Forward pass after conversion:
  Input: torch.Size([2, 32, 256])
  Output: torch.Size([2, 32, 256])
  Output finite: True

======================================================================
  FP8 Training
======================================================================
Setting up FP8 training...
✅ FP8 training setup complete
   Forward format: e4m3
   Backward format: e5m2
   Transformer Engine: ❌
   Hardware optimization: ❌

Training for 10 steps...
FP8 overflow detected at step 4, reducing scale to 512.0
  Step 5: loss=4.8634, success=True
  Step 10: loss=4.8447, success=True

Training Statistics:
  Steps: 0
  Overflows: 1
  FP8 enabled: True
  Final scale: 512.00

======================================================================
  FP8 Performance Benchmark
======================================================================
Device: cuda

Benchmarking FP8 vs Standard Linear layers:
    In    Out  Batch   FP8 (ms)   Std (ms)  Speedup
--------------------------------------------------
   512    512     32      0.501      0.058    0.12x
  1024   1024     32      0.551      0.088    0.16x
  2048   2048     16      0.491      0.054    0.11x

Note: FP8 native = True

======================================================================
  FP8 Numerical Stability
======================================================================
Testing FP8 quantization stability:
Case               Scale   E4M3 Error   E5M2 Error Recommended
------------------------------------------------------------
Normal range        1.00     0.000676     0.002727 E4M3      
Large values       10.00     0.072066     0.279707 E4M3      
Small values        0.01     0.000000     0.000000 E4M3      
Very large        100.00     7.012960    27.738632 E4M3      

Conclusion: E4M3 is generally preferred for activations (higher precision)
            E5M2 is better for gradients (wider dynamic range)

======================================================================
  Demo Summary
======================================================================
Results: 8/8 demos passed

  [PASS] availability
  [PASS] quantization
  [PASS] native_linear
  [PASS] inference_engine
  [PASS] model_conversion
  [PASS] training
  [PASS] benchmark
  [PASS] numerical_stability

FP8 Support Status:
  Native FP8 types: Available
  PyTorch version: 2.7.1+cu128
  GPU: NVIDIA L4
  Hardware FP8: Limited (Compute 8.9)
