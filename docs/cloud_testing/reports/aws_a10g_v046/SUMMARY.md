# AWS NVIDIA A10G Validation Report - v0.4.6

**Platform**: AWS EC2 (us-east-1)
**Instance**: g5.xlarge
**GPU**: NVIDIA A10G (24GB)
**Date**: 2026-01-19
**PyTorch Version**: 2.7.0+cu128
**CUDA Compute**: 8.6

---

## Test Results Summary

| Test Suite | Passed | Total | Status |
|------------|--------|-------|--------|
| NVIDIA Backend | 66 | 66 | ✅ PASS |
| MoE Tests | 48 | 48 | ✅ PASS |
| FP8 Native | 51 | 51 | ✅ PASS |
| FP8 Training | 10 | 20 | ⚠️ PARTIAL |
| **Total** | **175** | **185** | **94.6%** |

### FP8 Training Notes
10 tests failed due to Transformer Engine pydantic compatibility issue (`scaling_factor_compute_algo` validation error). This affects TE-based FP8 training but not native FP8 operations.

---

## Demo Results Summary

| Demo | Status | Details |
|------|--------|---------|
| MoE Demo | ✅ PASS | 8/8 demonstrations |
| FP8 Native Demo | ⚠️ PARTIAL | 7/8 demonstrations (training failed) |
| **Total** | **15/16** | **93.8%** |

---

## Performance Benchmarks

### MoE Throughput (8 experts, top-2)

| Hidden Size | Batch | Seq Len | Time/Batch | Throughput |
|-------------|-------|---------|------------|------------|
| 512 | 32 | 128 | 4.35ms | 940.8K tok/s |
| 1024 | 32 | 128 | 7.74ms | 529.4K tok/s |

### A10G vs L4 Comparison

| Config | A10G | L4 | A10G Speedup |
|--------|------|-----|--------------|
| 512d | 940.8K | 525.7K | 1.79x |
| 1024d | 529.4K | 311.0K | 1.70x |

---

## Hardware Information

```
GPU: NVIDIA A10G
Memory: 24GB
Compute Capability: 8.6
CUDA: 12.8
PyTorch: 2.7.0+cu128
```

---

## Files in This Report

- `gpu_info.txt` - GPU hardware information
- `nvidia_backend.txt` - NVIDIA backend test output
- `moe_tests.txt` - MoE test output
- `fp8_tests.txt` - FP8 test output
- `moe_demo.txt` - MoE demo output
- `fp8_demo.txt` - FP8 demo output
- `moe_benchmark.txt` - MoE performance benchmark

---

## Conclusion

v0.4.6 validation on AWS NVIDIA A10G shows strong results:
- 175/185 tests passing (94.6%)
- MoE tests 100% passing
- A10G provides ~1.7x higher throughput than L4
- Transformer Engine pydantic compatibility issue affects some FP8 training tests

---

*Report generated by KernelPyTorch v0.4.6 Cloud Validation*
