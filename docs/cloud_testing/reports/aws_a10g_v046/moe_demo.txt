/opt/pytorch/lib/python3.12/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/opt/pytorch/lib/python3.12/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ubuntu/kernel_pytorch/attention/distributed/ring_attention.py:41: UserWarning: Hardware Abstraction Layer not available - using basic device coordination
  warnings.warn("Hardware Abstraction Layer not available - using basic device coordination")
/home/ubuntu/kernel_pytorch/attention/distributed/context_parallel.py:41: UserWarning: Hardware Abstraction Layer not available - using basic device coordination
  warnings.warn("Hardware Abstraction Layer not available - using basic device coordination")
/home/ubuntu/kernel_pytorch/distributed_scale/__init__.py:28: FutureWarning: communication_optimization.py has been refactored into multiple focused modules. Consider importing from the specific modules directly: communication_primitives, network_optimization, communication_profiling
  from .communication_optimization import (
/home/ubuntu/kernel_pytorch/distributed_scale/__init__.py:35: FutureWarning: hardware_adaptation.py has been refactored into multiple focused modules. Consider importing from the specific modules directly: hardware_discovery, thermal_power_management, fault_tolerance, hardware_adapter
  from .hardware_adaptation import (
/home/ubuntu/kernel_pytorch/distributed_scale/__init__.py:42: FutureWarning: orchestration.py has been refactored into multiple focused modules. Consider importing from the specific modules directly: job_management, cluster_management, scaling_fault_tolerance
  from .orchestration import (
/home/ubuntu/kernel_pytorch/hardware/__init__.py:28: UserWarning: Hardware abstraction import failed: No module named 'kernel_pytorch.hardware.distributed_scale'
  warnings.warn(f"Hardware abstraction import failed: {e}")
============================================================
Mixture of Experts (MoE) Demonstrations
kernel_pytorch v0.4.6
============================================================

============================================================
Demo 1: Basic MoE Layer
============================================================
Config: 8 experts, top-2 routing
Created MoE layer with 8 experts
Input shape: torch.Size([4, 32, 256])
Output shape: torch.Size([4, 32, 256])

Using create_moe convenience function: 8 experts

============================================================
Demo 2: Different MoE Layer Types
============================================================
Standard MoE output: torch.Size([2, 16, 256])
Sparse MoE (25% sparsity) output: torch.Size([2, 16, 256])
Switch Transformer (top-1) output: torch.Size([2, 16, 256])
GLaM-style MoE output: torch.Size([2, 16, 256])
Adaptive MoE output: torch.Size([2, 16, 256])
  Factory 'standard': MoELayer
  Factory 'sparse': SparseMoELayer
  Factory 'switch': SwitchTransformerMoE
  Factory 'glam': GLaMStyleMoE
  Factory 'adaptive': AdaptiveMoELayer

============================================================
Demo 3: Routing Strategies
============================================================
TopK Router:
  Expert indices shape: torch.Size([100, 2])
  Expert weights shape: torch.Size([100, 2])
Switch Router (top-1):
  Expert indices shape: torch.Size([100, 1])
Hash Router:
  Deterministic expert assignment: [[7, 5], [5, 7], [1, 0], [0, 4], [0, 6]]
Learned Router:
  Router features shape: torch.Size([100, 128])
Dynamic Capacity Router:
  Complexity scores shape: torch.Size([100])
  Factory 'topk': TopKRouter
  Factory 'switch': SwitchRouter
  Factory 'hash': HashRouter
  Factory 'learned': LearnedRouter
  Factory 'dynamic': DynamicCapacityRouter

============================================================
Demo 4: Expert Network Architectures
============================================================
FeedForward Expert: 525,568 params, output shape: torch.Size([100, 256])
Convolutional Expert: 6,819,072 params, output shape: torch.Size([100, 256])
Attention Expert: 4,726,016 params, output shape: torch.Size([100, 256])
Parameter-Efficient Expert: 567,552 params, output shape: torch.Size([100, 256])

============================================================
Demo 5: Load Balancing
============================================================
Expert capacities: [157, 157, 157, 157, 157, 157, 157, 157]
Tokens dropped: 0
Load balance loss (switch): 2.0006
Assignment entropy: 2.077
Max/Min assignment ratio: 1.11/0.88

MoE with auxiliary losses:
  load_balance_loss: 0.020012
  router_z_loss: 0.004613
  tokens_dropped: 1.000000

============================================================
Demo 6: Training MoE Models
============================================================
Training for 5 steps...
  Step 1: main_loss=2.2954, aux_loss=9.024601
  Step 2: main_loss=2.3017, aux_loss=5.024433
  Step 3: main_loss=2.3209, aux_loss=13.024579
  Step 4: main_loss=2.3032, aux_loss=3.024455
  Step 5: main_loss=2.3620, aux_loss=9.024454

Expert utilization after training:
  Balance: 0.998
  Efficiency: 0.250

============================================================
Demo 7: MoE in Transformer
============================================================
MoE Transformer:
  Input: torch.Size([2, 32])
  Output logits: torch.Size([2, 32, 1000])
  Auxiliary losses: 8 items
  Total params: 9,585,640
  MoE params: 8,413,184 (87.8%)

============================================================
Demo 8: Performance Comparison
============================================================
Input shape: (8, 128, 512)

Standard FFN:
  Parameters: 2,099,712
  Time: 36.41 ms

MoE (8 experts, top-2):
  Parameters: 16,801,792
  Time: 87.76 ms
  Parameter ratio: 8.0x
  Time ratio: 2.4x

============================================================
All demos completed!
============================================================
