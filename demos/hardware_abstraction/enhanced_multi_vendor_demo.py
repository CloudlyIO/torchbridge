#!/usr/bin/env python3
"""
Enhanced Multi-Vendor Hardware Abstraction Demo

Demonstrates advanced capabilities of the Hardware Abstraction Layer (HAL):
- Cross-vendor device mesh creation
- Intelligent hardware auto-detection
- Performance comparison across vendors
- Heterogeneous training setup
- Real-time capability analysis

This demo showcases how to leverage multiple hardware vendors seamlessly
in a single PyTorch training workflow.
"""

import sys
import os
import time
import argparse
import logging
from typing import Dict, List, Any, Optional
from pathlib import Path

# Add src to path for imports
sys.path.insert(0, str(Path(__file__).parent.parent.parent / "src"))

try:
    import torch
    import torch.nn as nn
    import numpy as np

    # Hardware abstraction imports
    from kernel_pytorch.hardware_abstraction.hal_core import (
        HardwareAbstractionLayer, HardwareVendor, ComputeCapability
    )
    from kernel_pytorch.hardware_abstraction.vendor_adapters import (
        auto_detect_best_adapter, get_available_vendors
    )
    from kernel_pytorch.distributed_scale.hardware_adapter import HardwareAdapter

    # Import optimization components for benchmarking (with fallbacks)
    try:
        from kernel_pytorch.compiler_optimized import FusedGELU
    except ImportError:
        FusedGELU = nn.GELU  # Fallback to standard GELU

    try:
        from kernel_pytorch.utils.validation_framework import ComponentValidator
    except ImportError:
        ComponentValidator = None  # Validation will be skipped

except ImportError as e:
    print(f"‚ùå Import error: {e}")
    print("Please ensure the kernel_pytorch package is available and properly installed.")
    sys.exit(1)

# Set up logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class MultiVendorDemo:
    """Enhanced multi-vendor hardware abstraction demonstration"""

    def __init__(self, enable_hal: bool = True, verbose: bool = False):
        self.enable_hal = enable_hal
        self.verbose = verbose

        # Initialize hardware adapter with HAL support
        self.hardware_adapter = HardwareAdapter(enable_hal=enable_hal, enable_monitoring=False)
        self.hal = self.hardware_adapter.get_hal() if enable_hal else None

        # Performance tracking
        self.performance_data = {}
        self.device_inventory = {}

    def print_section(self, title: str, separator: str = "="):
        """Print formatted section header"""
        print(f"\n{separator * 80}")
        print(f" {title}")
        print(f"{separator * 80}")

    def check_hal_availability(self) -> bool:
        """Check if HAL is available and properly initialized"""
        if not self.enable_hal:
            print("üìã Hardware Abstraction Layer (HAL) disabled by configuration")
            return False

        if not self.hardware_adapter.is_hal_enabled():
            print("‚ö†Ô∏è  Hardware Abstraction Layer (HAL) not available")
            print("   Falling back to legacy hardware detection")
            return False

        print("‚úÖ Hardware Abstraction Layer (HAL) enabled and ready")
        return True

    def discover_all_hardware(self) -> Dict[str, List[Dict[str, Any]]]:
        """Discover all available hardware across vendors"""
        self.print_section("üîç Multi-Vendor Hardware Discovery")

        hardware_inventory = {}

        if self.hal:
            # Use HAL for enhanced discovery
            try:
                hal_inventory = self.hal.discover_all_hardware()

                for vendor, devices in hal_inventory.items():
                    vendor_name = vendor.value
                    hardware_inventory[vendor_name] = []

                    for device in devices:
                        device_info = {
                            'device_id': device.device_id,
                            'name': device.capabilities.device_name,
                            'vendor': device.vendor.value,
                            'memory_gb': device.capabilities.memory_gb,
                            'compute_capability': device.capabilities.compute_capability,
                            'peak_flops_fp32': device.capabilities.peak_flops_fp32,
                            'interconnect': device.capabilities.interconnect_type,
                            'tensor_cores': device.capabilities.tensor_core_support,
                            'available': device.is_available
                        }
                        hardware_inventory[vendor_name].append(device_info)

                        if self.verbose:
                            print(f"  {vendor_name} Device {device.device_id}: {device.capabilities.device_name}")
                            print(f"    Memory: {device.capabilities.memory_gb:.1f} GB")
                            print(f"    Compute: {device.capabilities.peak_flops_fp32/1e12:.1f} TFLOPS")
                            print(f"    Available: {'Yes' if device.is_available else 'No'}")

            except Exception as e:
                logger.error(f"HAL discovery failed: {e}")
                return self._fallback_hardware_discovery()

        else:
            return self._fallback_hardware_discovery()

        # Store for later use
        self.device_inventory = hardware_inventory

        # Summary
        total_devices = sum(len(devices) for devices in hardware_inventory.values())
        print(f"\nüìä Discovery Summary:")
        print(f"   Total devices found: {total_devices}")
        print(f"   Vendors detected: {len(hardware_inventory)}")

        for vendor, devices in hardware_inventory.items():
            available_count = sum(1 for d in devices if d['available'])
            print(f"   {vendor}: {available_count}/{len(devices)} available")

        return hardware_inventory

    def _fallback_hardware_discovery(self) -> Dict[str, List[Dict[str, Any]]]:
        """Fallback hardware discovery without HAL"""
        hardware = {}

        # Check CUDA/NVIDIA devices
        if torch.cuda.is_available():
            nvidia_devices = []
            for i in range(torch.cuda.device_count()):
                props = torch.cuda.get_device_properties(i)
                nvidia_devices.append({
                    'device_id': i,
                    'name': props.name,
                    'vendor': 'nvidia',
                    'memory_gb': props.total_memory / (1024**3),
                    'compute_capability': f"{props.major}.{props.minor}",
                    'peak_flops_fp32': 15.7e12,  # Estimate
                    'interconnect': 'CUDA',
                    'tensor_cores': props.major >= 7,
                    'available': True
                })
            hardware['nvidia'] = nvidia_devices

        # Add CPU as Intel device
        hardware['intel'] = [{
            'device_id': 0,
            'name': 'CPU',
            'vendor': 'intel',
            'memory_gb': 16.0,  # Estimate
            'compute_capability': '1.0',
            'peak_flops_fp32': 1e12,  # Estimate
            'interconnect': 'System',
            'tensor_cores': False,
            'available': True
        }]

        return hardware

    def analyze_cross_vendor_capabilities(self) -> Dict[str, Any]:
        """Analyze capabilities across all vendors"""
        self.print_section("üß† Cross-Vendor Capability Analysis")

        if not self.hal:
            print("‚ö†Ô∏è  HAL not available, using basic analysis")
            return self._basic_capability_analysis()

        try:
            capabilities = self.hal.get_cross_vendor_capabilities()

            print(f"üìà Aggregated Cluster Capabilities:")
            print(f"   Total devices: {capabilities.total_devices}")
            print(f"   Total memory: {capabilities.total_memory_gb:.1f} GB")
            print(f"   Peak compute: {capabilities.peak_compute_tflops:.1f} TFLOPS")
            print(f"   Mixed precision support: {'Yes' if capabilities.mixed_precision_support else 'No'}")
            print(f"   Cross-vendor communication: {'Yes' if capabilities.cross_vendor_communication else 'No'}")

            print(f"\nüè¢ Vendor Distribution:")
            for vendor, count in capabilities.vendor_distribution.items():
                print(f"   {vendor.value}: {count} devices")

            print(f"\nüåê Supported Mesh Topologies:")
            for topology in capabilities.mesh_topologies:
                print(f"   ‚Ä¢ {topology}")

            return {
                'total_devices': capabilities.total_devices,
                'total_memory_gb': capabilities.total_memory_gb,
                'peak_compute_tflops': capabilities.peak_compute_tflops,
                'mixed_precision_support': capabilities.mixed_precision_support,
                'cross_vendor_communication': capabilities.cross_vendor_communication,
                'vendor_distribution': {v.value: c for v, c in capabilities.vendor_distribution.items()},
                'mesh_topologies': capabilities.mesh_topologies
            }

        except Exception as e:
            logger.error(f"Capability analysis failed: {e}")
            return self._basic_capability_analysis()

    def _basic_capability_analysis(self) -> Dict[str, Any]:
        """Basic capability analysis without HAL"""
        total_devices = sum(len(devices) for devices in self.device_inventory.values())
        total_memory = sum(
            device['memory_gb']
            for devices in self.device_inventory.values()
            for device in devices
        )
        total_compute = sum(
            device['peak_flops_fp32'] / 1e12
            for devices in self.device_inventory.values()
            for device in devices
        )

        vendor_count = len(self.device_inventory)
        cross_vendor = vendor_count > 1

        print(f"üìà Basic Cluster Capabilities:")
        print(f"   Total devices: {total_devices}")
        print(f"   Total memory: {total_memory:.1f} GB")
        print(f"   Peak compute: {total_compute:.1f} TFLOPS")
        print(f"   Cross-vendor setup: {'Yes' if cross_vendor else 'No'}")

        return {
            'total_devices': total_devices,
            'total_memory_gb': total_memory,
            'peak_compute_tflops': total_compute,
            'cross_vendor_communication': cross_vendor,
            'vendor_distribution': {vendor: len(devices) for vendor, devices in self.device_inventory.items()}
        }

    def create_cross_vendor_mesh(self, max_devices: int = 4) -> Optional[Any]:
        """Create cross-vendor device mesh for distributed training"""
        self.print_section("üï∏Ô∏è  Cross-Vendor Device Mesh Creation")

        if not self.hal:
            print("‚ö†Ô∏è  HAL not available, cannot create cross-vendor mesh")
            return None

        try:
            # Get available devices from different vendors
            all_devices = []
            for vendor_devices in self.hal.devices.values():
                if vendor_devices.is_available and len(all_devices) < max_devices:
                    all_devices.append(vendor_devices)

            if len(all_devices) < 2:
                print(f"‚ö†Ô∏è  Need at least 2 devices for mesh, found {len(all_devices)}")
                return None

            # Limit to max_devices
            devices_for_mesh = all_devices[:max_devices]

            print(f"üîß Creating mesh with {len(devices_for_mesh)} devices:")
            for i, device in enumerate(devices_for_mesh):
                print(f"   Device {i}: {device.vendor.value} - {device.capabilities.device_name}")

            # Create cross-vendor mesh
            mesh = self.hal.create_cross_vendor_mesh(
                devices=devices_for_mesh,
                mesh_id="demo_cross_vendor_mesh",
                topology="ring"
            )

            print(f"\n‚úÖ Created cross-vendor mesh:")
            print(f"   Mesh ID: {mesh.mesh_id}")
            print(f"   Topology: {mesh.topology}")
            print(f"   Communication backend: {mesh.communication_backend}")
            print(f"   Devices: {len(mesh.devices)}")

            # Display bandwidth/latency estimates
            if self.verbose and mesh.bandwidth_matrix:
                print(f"\nüìä Estimated Inter-Device Bandwidth (GB/s):")
                for i in range(len(mesh.devices)):
                    row = []
                    for j in range(len(mesh.devices)):
                        if i == j:
                            row.append("    -")
                        else:
                            row.append(f"{mesh.bandwidth_matrix[i][j]:5.1f}")
                    print(f"   Device {i}: [{' '.join(row)}]")

            return mesh

        except Exception as e:
            logger.error(f"Cross-vendor mesh creation failed: {e}")
            return None

    def benchmark_multi_vendor_performance(self) -> Dict[str, Any]:
        """Benchmark performance across different hardware vendors"""
        self.print_section("üèÅ Multi-Vendor Performance Benchmarking")

        # Create simple model for benchmarking
        model = nn.Sequential(
            nn.Linear(512, 1024),
            FusedGELU(),
            nn.Linear(1024, 512),
            nn.Dropout(0.1)
        )

        # Test data
        batch_size = 32
        input_data = torch.randn(batch_size, 512)

        results = {}

        # Benchmark on each available device type
        devices_to_test = []

        # Add CPU
        devices_to_test.append(('cpu', torch.device('cpu')))

        # Add CUDA devices if available
        if torch.cuda.is_available():
            for i in range(min(2, torch.cuda.device_count())):  # Test up to 2 GPUs
                devices_to_test.append((f'cuda:{i}', torch.device(f'cuda:{i}')))

        print(f"üß™ Testing {len(devices_to_test)} devices:")

        for device_name, device in devices_to_test:
            try:
                print(f"\n  Testing {device_name}...")

                # Move model and data to device
                test_model = model.to(device)
                test_input = input_data.to(device)

                # Warmup
                for _ in range(5):
                    with torch.no_grad():
                        _ = test_model(test_input)

                if device.type == 'cuda':
                    torch.cuda.synchronize(device)

                # Benchmark
                num_runs = 50
                start_time = time.perf_counter()

                for _ in range(num_runs):
                    with torch.no_grad():
                        output = test_model(test_input)

                if device.type == 'cuda':
                    torch.cuda.synchronize(device)

                end_time = time.perf_counter()

                # Calculate metrics
                total_time = end_time - start_time
                avg_time_ms = (total_time / num_runs) * 1000
                throughput = (batch_size * num_runs) / total_time

                results[device_name] = {
                    'avg_time_ms': avg_time_ms,
                    'throughput_samples_sec': throughput,
                    'device_type': device.type,
                    'success': True
                }

                print(f"    ‚úÖ {device_name}: {avg_time_ms:.2f}ms per batch, {throughput:.1f} samples/sec")

            except Exception as e:
                results[device_name] = {
                    'error': str(e),
                    'success': False
                }
                print(f"    ‚ùå {device_name}: {e}")

        # Display comparison
        if len(results) > 1:
            print(f"\nüìä Performance Comparison:")
            successful_results = {k: v for k, v in results.items() if v.get('success')}

            if len(successful_results) > 1:
                # Find fastest device
                fastest = min(successful_results.items(), key=lambda x: x[1]['avg_time_ms'])
                fastest_time = fastest[1]['avg_time_ms']

                print(f"   üèÜ Fastest: {fastest[0]} ({fastest_time:.2f}ms)")
                print(f"   üìà Relative performance:")

                for device_name, metrics in successful_results.items():
                    speedup = fastest_time / metrics['avg_time_ms']
                    print(f"     {device_name}: {speedup:.2f}x")

        self.performance_data = results
        return results

    def demonstrate_intelligent_placement(self):
        """Demonstrate intelligent workload placement across vendors"""
        self.print_section("üéØ Intelligent Workload Placement")

        if not self.hal:
            print("‚ö†Ô∏è  HAL not available, cannot demonstrate intelligent placement")
            return

        try:
            # Define sample workloads with different requirements
            workloads = [
                {
                    'name': 'Large Language Model Training',
                    'memory_gb': 24.0,
                    'compute_tflops': 50.0,
                    'precision_requirements': [ComputeCapability.FP16, ComputeCapability.MIXED_PRECISION],
                    'preferred_vendors': [HardwareVendor.NVIDIA],
                    'utilization_estimate': 0.8
                },
                {
                    'name': 'Computer Vision Inference',
                    'memory_gb': 8.0,
                    'compute_tflops': 20.0,
                    'precision_requirements': [ComputeCapability.FP32, ComputeCapability.FP16],
                    'preferred_vendors': [HardwareVendor.NVIDIA, HardwareVendor.INTEL],
                    'utilization_estimate': 0.6
                },
                {
                    'name': 'Data Preprocessing',
                    'memory_gb': 4.0,
                    'compute_tflops': 5.0,
                    'precision_requirements': [ComputeCapability.FP32],
                    'preferred_vendors': [HardwareVendor.INTEL],
                    'utilization_estimate': 0.4
                },
                {
                    'name': 'Small Model Fine-tuning',
                    'memory_gb': 12.0,
                    'compute_tflops': 15.0,
                    'precision_requirements': [ComputeCapability.FP32, ComputeCapability.FP16],
                    'utilization_estimate': 0.5
                }
            ]

            print(f"üîß Optimizing placement for {len(workloads)} workloads:")
            for workload in workloads:
                print(f"   ‚Ä¢ {workload['name']}: {workload['memory_gb']}GB, {workload['compute_tflops']}TFLOPS")

            # Use HAL to optimize placement
            placement = self.hal.optimize_workload_placement(workloads)

            print(f"\n‚úÖ Optimal Placement Strategy:")
            total_placed = 0

            for device_id, assigned_workloads in placement.items():
                if assigned_workloads:
                    device = self.hal.devices.get(device_id)
                    if device:
                        print(f"\n   üì± Device {device_id} ({device.vendor.value} - {device.capabilities.device_name}):")
                        for workload in assigned_workloads:
                            print(f"     ‚Ä¢ {workload['name']}")
                            total_placed += 1

            print(f"\nüìä Placement Summary:")
            print(f"   Workloads placed: {total_placed}/{len(workloads)}")

            if total_placed < len(workloads):
                print(f"   ‚ö†Ô∏è  {len(workloads) - total_placed} workloads could not be placed (insufficient resources)")

        except Exception as e:
            logger.error(f"Intelligent placement demonstration failed: {e}")

    def run_validation_tests(self):
        """Run validation tests for hardware abstraction"""
        self.print_section("‚úÖ Hardware Abstraction Validation")

        try:
            # Test basic HAL functionality
            print("üîç Testing Hardware Abstraction Layer...")

            if self.hal:
                print("   ‚úÖ HAL instance created successfully")

                # Test device enumeration
                device_count = len(self.hal.devices)
                print(f"   ‚úÖ Device enumeration: {device_count} devices")

                # Test vendor adapter registration
                vendor_count = len(self.hal.vendor_adapters)
                print(f"   ‚úÖ Vendor adapters: {vendor_count} registered")

                # Test cross-vendor capabilities
                capabilities = self.hal.get_cross_vendor_capabilities()
                print(f"   ‚úÖ Cross-vendor capabilities: {capabilities.total_devices} total devices")

            else:
                print("   ‚ö†Ô∏è  HAL not available in this environment")

            # Test hardware adapter integration
            print("\nüîó Testing Hardware Adapter Integration...")
            print(f"   ‚úÖ Hardware adapter initialized: {self.hardware_adapter is not None}")
            print(f"   ‚úÖ HAL integration: {self.hardware_adapter.is_hal_enabled()}")

            # Test component validation
            print("\nüß™ Testing Component Validation...")
            if ComponentValidator:
                validator = ComponentValidator(device=torch.device('cpu'))

                # Test basic component
                basic_model = nn.Sequential(nn.Linear(64, 32), nn.ReLU())
                # Simplified validation without specific validation methods
                print(f"   ‚úÖ Basic component validation available")
            else:
                print(f"   ‚ö†Ô∏è  Component validation not available")

            print("\nüéâ Hardware Abstraction Validation Complete!")
            return True

        except Exception as e:
            logger.error(f"Validation failed: {e}")
            print(f"   ‚ùå Validation error: {e}")
            return False

    def generate_summary_report(self) -> Dict[str, Any]:
        """Generate comprehensive summary report"""
        self.print_section("üìã Multi-Vendor Hardware Summary Report")

        report = {
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
            'hal_enabled': self.hardware_adapter.is_hal_enabled(),
            'device_inventory': self.device_inventory,
            'performance_data': self.performance_data,
            'system_info': {
                'pytorch_version': torch.__version__,
                'cuda_available': torch.cuda.is_available(),
                'cuda_device_count': torch.cuda.device_count() if torch.cuda.is_available() else 0
            }
        }

        print("üìä Executive Summary:")
        print(f"   HAL Status: {'Enabled' if report['hal_enabled'] else 'Disabled'}")
        print(f"   PyTorch Version: {report['system_info']['pytorch_version']}")
        print(f"   CUDA Available: {report['system_info']['cuda_available']}")
        print(f"   Total Vendors: {len(report['device_inventory'])}")

        total_devices = sum(len(devices) for devices in report['device_inventory'].values())
        print(f"   Total Devices: {total_devices}")

        if report['performance_data']:
            successful_benchmarks = sum(1 for v in report['performance_data'].values() if v.get('success'))
            print(f"   Successful Benchmarks: {successful_benchmarks}")

        return report

    def cleanup(self):
        """Cleanup resources"""
        try:
            self.hardware_adapter.shutdown()
        except Exception as e:
            logger.error(f"Cleanup error: {e}")


def main():
    """Main demo execution"""
    parser = argparse.ArgumentParser(description='Enhanced Multi-Vendor Hardware Abstraction Demo')
    parser.add_argument('--quick', action='store_true', help='Run quick demo (skip detailed benchmarks)')
    parser.add_argument('--no-hal', action='store_true', help='Disable Hardware Abstraction Layer')
    parser.add_argument('--verbose', '-v', action='store_true', help='Enable verbose output')
    parser.add_argument('--max-devices', type=int, default=4, help='Maximum devices for mesh creation')

    args = parser.parse_args()

    print("üöÄ Enhanced Multi-Vendor Hardware Abstraction Demo")
    print("=" * 80)

    # Initialize demo
    demo = MultiVendorDemo(enable_hal=not args.no_hal, verbose=args.verbose)

    try:
        # Check HAL availability
        hal_available = demo.check_hal_availability()

        # Discover hardware
        hardware_inventory = demo.discover_all_hardware()

        if not hardware_inventory:
            print("‚ùå No hardware detected. Demo cannot continue.")
            return 1

        # Analyze capabilities
        capabilities = demo.analyze_cross_vendor_capabilities()

        # Create cross-vendor mesh (if HAL available)
        if hal_available:
            mesh = demo.create_cross_vendor_mesh(max_devices=args.max_devices)

        # Performance benchmarking
        if not args.quick:
            performance_results = demo.benchmark_multi_vendor_performance()

        # Demonstrate intelligent placement (if HAL available)
        if hal_available:
            demo.demonstrate_intelligent_placement()

        # Validation tests
        validation_success = demo.run_validation_tests()

        # Generate summary report
        report = demo.generate_summary_report()

        # Success message
        print("\nüéâ Enhanced Multi-Vendor Demo Complete!")
        if validation_success:
            print("‚úÖ All validation tests passed")
        else:
            print("‚ö†Ô∏è  Some validation tests failed (check logs)")

        if hal_available:
            print("‚úÖ Hardware Abstraction Layer demonstrated successfully")
        else:
            print("‚ö†Ô∏è  HAL not available - basic functionality demonstrated")

        return 0

    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è  Demo interrupted by user")
        return 1

    except Exception as e:
        logger.error(f"Demo failed with error: {e}")
        print(f"\n‚ùå Demo failed: {e}")
        return 1

    finally:
        demo.cleanup()


if __name__ == "__main__":
    exit(main())