#!/usr/bin/env python3
"""
Semantic ML Models Demo

Demonstrates how kernel-optimized components maintain semantic clarity
while showcasing different ML architectures and concepts.
"""

import sys
import os
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))

import torch
import torch.nn as nn
import torch.nn.functional as F
import math
import numpy as np
import time

from kernel_pytorch.components.basic_optimized import (
    OptimizedTransformerBlock,
    OptimizedMultiHeadAttention,
    OptimizedLayerNorm,
    OptimizedMLP,
    PositionalEncoding
)

print("üß† Semantic ML Models with Kernel Optimization")
print("="*60)

class LanguageModel(nn.Module):
    """
    Autoregressive language model demonstrating:
    - Token prediction and sequence generation
    - Causal attention patterns
    - Gradient flow in deep networks
    """

    def __init__(self, vocab_size=1000, dim=256, num_layers=4, num_heads=8):
        super().__init__()
        self.vocab_size = vocab_size
        self.dim = dim

        # Token embeddings
        self.token_embedding = nn.Embedding(vocab_size, dim)
        self.position_embedding = PositionalEncoding(dim)

        # Transformer layers with kernel optimization
        self.layers = nn.ModuleList([
            OptimizedTransformerBlock(dim, num_heads)
            for _ in range(num_layers)
        ])

        self.norm = OptimizedLayerNorm(dim)
        self.output_projection = nn.Linear(dim, vocab_size)

    def forward(self, input_ids):
        """Autoregressive forward pass"""
        # Embedding lookup - efficient for sparse indices
        x = self.token_embedding(input_ids)
        x = self.position_embedding(x)

        # Pass through optimized transformer layers
        for layer in self.layers:
            x = layer(x)

        x = self.norm(x)
        logits = self.output_projection(x)
        return logits

    @torch.no_grad()
    def generate(self, prompt_ids, max_length=50):
        """Demonstrate autoregressive generation"""
        self.eval()
        generated = prompt_ids.clone()

        for _ in range(max_length):
            # Get logits for current sequence
            logits = self.forward(generated)

            # Sample next token (greedy for simplicity)
            next_token = logits[:, -1].argmax(dim=-1, keepdim=True)

            # Append to sequence
            generated = torch.cat([generated, next_token], dim=1)

        return generated


class VisionTransformer(nn.Module):
    """
    Vision transformer demonstrating:
    - Patch-based image processing
    - Spatial positional encoding
    - Global attention across patches
    """

    def __init__(self, image_size=64, patch_size=8, num_classes=10, dim=256, num_layers=4, num_heads=8):
        super().__init__()
        self.patch_size = patch_size
        self.num_patches = (image_size // patch_size) ** 2

        # Patch embedding - convert image patches to tokens
        self.patch_embedding = nn.Conv2d(3, dim, kernel_size=patch_size, stride=patch_size)

        # Positional embeddings for spatial awareness
        self.pos_embedding = nn.Parameter(torch.randn(1, self.num_patches + 1, dim))

        # Classification token for global representation
        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))

        # Transformer layers optimized for vision
        self.layers = nn.ModuleList([
            OptimizedTransformerBlock(dim, num_heads)
            for _ in range(num_layers)
        ])

        self.norm = OptimizedLayerNorm(dim)
        self.classifier = nn.Linear(dim, num_classes)

    def forward(self, images):
        """Process images through patch attention"""
        batch_size = images.shape[0]

        # Convert to patches - efficient kernel operation
        patches = self.patch_embedding(images)  # [B, dim, H/P, W/P]
        patches = patches.flatten(2).transpose(1, 2)  # [B, num_patches, dim]

        # Add classification token
        cls_tokens = self.cls_token.expand(batch_size, -1, -1)
        tokens = torch.cat([cls_tokens, patches], dim=1)

        # Add positional encoding for spatial awareness
        tokens = tokens + self.pos_embedding

        # Pass through transformer with spatial attention
        for layer in self.layers:
            tokens = layer(tokens)

        tokens = self.norm(tokens)

        # Extract global representation from CLS token
        cls_representation = tokens[:, 0]
        logits = self.classifier(cls_representation)

        return logits, cls_representation


class GraphNeuralNetwork(nn.Module):
    """
    Graph neural network demonstrating:
    - Node and edge representations
    - Message passing between connected nodes
    - Graph-level aggregation
    """

    def __init__(self, node_features=32, hidden_dim=128, num_classes=5, num_layers=3):
        super().__init__()
        self.hidden_dim = hidden_dim

        # Node embedding
        self.node_embedding = nn.Linear(node_features, hidden_dim)

        # Message passing layers using attention
        self.message_layers = nn.ModuleList([
            GraphAttentionLayer(hidden_dim, num_heads=4)
            for _ in range(num_layers)
        ])

        # Graph-level classification
        self.classifier = nn.Sequential(
            OptimizedLayerNorm(hidden_dim),
            OptimizedMLP(hidden_dim),
            nn.Linear(hidden_dim, num_classes)
        )

    def forward(self, node_features, edge_index):
        """Message passing with attention"""
        # Embed nodes
        node_embeddings = self.node_embedding(node_features)

        # Message passing through graph layers
        for layer in self.message_layers:
            node_embeddings = layer(node_embeddings, edge_index)

        # Global pooling (simple mean for demo)
        graph_representation = node_embeddings.mean(dim=0)

        # Classification
        logits = self.classifier(graph_representation)
        return logits, node_embeddings


class GraphAttentionLayer(nn.Module):
    """Simple graph attention for message passing demonstration"""

    def __init__(self, dim, num_heads=4):
        super().__init__()
        self.dim = dim
        self.num_heads = num_heads
        self.head_dim = dim // num_heads

        self.q_proj = nn.Linear(dim, dim)
        self.k_proj = nn.Linear(dim, dim)
        self.v_proj = nn.Linear(dim, dim)
        self.out_proj = nn.Linear(dim, dim)

        self.norm = OptimizedLayerNorm(dim)
        self.mlp = OptimizedMLP(dim)

    def forward(self, node_features, edge_index):
        """Simplified message passing"""
        residual = node_features
        num_nodes = node_features.shape[0]

        # Self-attention on all nodes (simplified)
        # In practice, this would use edge_index for neighbor selection
        q = self.q_proj(node_features).view(num_nodes, self.num_heads, self.head_dim)
        k = self.k_proj(node_features).view(num_nodes, self.num_heads, self.head_dim)
        v = self.v_proj(node_features).view(num_nodes, self.num_heads, self.head_dim)

        # Attention computation
        scale = self.head_dim ** -0.5
        scores = torch.matmul(q, k.transpose(-2, -1)) * scale
        weights = F.softmax(scores, dim=-1)
        output = torch.matmul(weights, v)

        # Reshape and project
        output = output.view(num_nodes, self.dim)
        output = self.out_proj(output)

        # Residual connection and MLP
        output = self.norm(output + residual)
        output = output + self.mlp(output)

        return output


def demonstrate_language_model():
    """Demonstrate autoregressive language modeling"""
    print("\nüó£Ô∏è  LANGUAGE MODEL (Autoregressive Generation)")
    print("-" * 50)

    vocab_size = 1000
    model = LanguageModel(vocab_size, dim=256, num_layers=3)
    model.eval()

    # Example input sequence
    batch_size, seq_len = 2, 32
    input_ids = torch.randint(0, vocab_size, (batch_size, seq_len))

    print(f"   üìù Input shape: {input_ids.shape}")

    # Forward pass
    with torch.no_grad():
        logits = model(input_ids)
        print(f"   üìä Output logits: {logits.shape}")
        print(f"   üéØ Vocabulary predictions for next token")

        # Text generation demo
        prompt = torch.randint(0, vocab_size, (1, 5))
        generated = model.generate(prompt, max_length=10)
        print(f"   üöÄ Generated sequence length: {generated.shape[1]}")

    print("   ‚úÖ Demonstrates: Causal attention, autoregressive modeling")


def demonstrate_vision_transformer():
    """Demonstrate vision transformer for image classification"""
    print("\nüëÅÔ∏è  VISION TRANSFORMER (Spatial Attention)")
    print("-" * 50)

    model = VisionTransformer(image_size=64, patch_size=8, num_classes=10)
    model.eval()

    # Example images
    batch_size = 4
    images = torch.randn(batch_size, 3, 64, 64)

    print(f"   üñºÔ∏è  Input shape: {images.shape}")

    # Forward pass
    with torch.no_grad():
        logits, cls_representation = model(images)
        print(f"   üìä Classification logits: {logits.shape}")
        print(f"   üéØ Global representation: {cls_representation.shape}")
        print(f"   üîç Number of patches: {model.num_patches}")

    print("   ‚úÖ Demonstrates: Patch embedding, spatial attention, global pooling")


def demonstrate_graph_neural_network():
    """Demonstrate graph neural network for node/graph classification"""
    print("\nüï∏Ô∏è  GRAPH NEURAL NETWORK (Message Passing)")
    print("-" * 50)

    model = GraphNeuralNetwork(node_features=32, hidden_dim=128, num_classes=5)
    model.eval()

    # Example graph
    num_nodes = 20
    num_edges = 40

    node_features = torch.randn(num_nodes, 32)
    edge_index = torch.randint(0, num_nodes, (2, num_edges))

    print(f"   üîó Nodes: {num_nodes}, Edges: {num_edges}")
    print(f"   üìä Node features: {node_features.shape}")

    # Forward pass
    with torch.no_grad():
        logits, node_embeddings = model(node_features, edge_index)
        print(f"   üéØ Graph classification: {logits.shape}")
        print(f"   üìç Node embeddings: {node_embeddings.shape}")

    print("   ‚úÖ Demonstrates: Message passing, graph pooling, relational learning")


def demonstrate_semantic_equivalence():
    """Show that optimizations preserve semantic meaning"""
    print("\nüîç SEMANTIC EQUIVALENCE VERIFICATION")
    print("-" * 50)

    # Test with language model
    vocab_size = 100
    model1 = LanguageModel(vocab_size, dim=128, num_layers=2)

    # Create a "different" optimization level (same model for demo)
    model2 = LanguageModel(vocab_size, dim=128, num_layers=2)
    model2.load_state_dict(model1.state_dict())  # Same weights

    input_ids = torch.randint(0, vocab_size, (1, 16))

    with torch.no_grad():
        output1 = model1(input_ids)
        output2 = model2(input_ids)

        # Check equivalence
        diff = torch.abs(output1 - output2)
        max_diff = diff.max().item()
        mean_diff = diff.mean().item()

        print(f"   üìè Maximum difference: {max_diff:.2e}")
        print(f"   üìä Mean difference: {mean_diff:.2e}")

        if max_diff < 1e-6:
            print("   ‚úÖ PASS: Semantic equivalence maintained")
        else:
            print("   ‚ö†Ô∏è  WARN: Small numerical differences detected")

    print("   üéØ Key insight: Optimizations preserve mathematical semantics")


def demonstrate_kernel_concepts():
    """Show kernel-level optimization concepts"""
    print("\n‚ö° KERNEL OPTIMIZATION CONCEPTS")
    print("-" * 50)

    # 1. Batching efficiency
    print("\n   1. Batching Effects:")
    model = OptimizedTransformerBlock(256, 8)

    # Small batch
    small_batch = torch.randn(1, 32, 256)
    start_time = time.perf_counter()
    for _ in range(10):
        _ = model(small_batch)
    small_time = time.perf_counter() - start_time

    # Large batch
    large_batch = torch.randn(8, 32, 256)
    start_time = time.perf_counter()
    for _ in range(10):
        _ = model(large_batch)
    large_time = time.perf_counter() - start_time

    efficiency = (large_batch.numel() / small_batch.numel()) / (large_time / small_time)
    print(f"      üî∏ Small batch (1): {small_time*100:.2f}ms")
    print(f"      üîπ Large batch (8): {large_time*100:.2f}ms")
    print(f"      üìà Batch efficiency: {efficiency:.2f}x")

    # 2. Memory layout importance
    print("\n   2. Memory Layout:")
    x = torch.randn(100, 512, 256)

    # Contiguous operations
    start_time = time.perf_counter()
    for _ in range(20):
        y = x.view(100, -1)  # Contiguous reshape
        _ = torch.sum(y, dim=1)
    contiguous_time = time.perf_counter() - start_time

    # Non-contiguous operations
    start_time = time.perf_counter()
    for _ in range(20):
        y = x.transpose(1, 2)  # Non-contiguous
        _ = torch.sum(y, dim=1)
    transpose_time = time.perf_counter() - start_time

    print(f"      üî∏ Contiguous ops: {contiguous_time*100:.2f}ms")
    print(f"      üîπ Transpose ops: {transpose_time*100:.2f}ms")
    print(f"      üìà Layout impact: {transpose_time/contiguous_time:.2f}x")


def main():
    """Main demonstration"""

    # Demonstrate different ML model semantics
    demonstrate_language_model()
    demonstrate_vision_transformer()
    demonstrate_graph_neural_network()

    # Show semantic preservation
    demonstrate_semantic_equivalence()

    # Kernel concepts
    demonstrate_kernel_concepts()

    # Summary
    print(f"\nüéì EDUCATIONAL SUMMARY")
    print("="*50)
    print("   1. üó£Ô∏è  Language Model: Autoregressive generation with causal attention")
    print("   2. üëÅÔ∏è  Vision Transformer: Spatial reasoning with patch-based attention")
    print("   3. üï∏Ô∏è  Graph Networks: Relational learning through message passing")
    print("   4. ‚ö° Kernel Optimization: Efficiency without sacrificing semantics")
    print("   5. üîç Semantic Equivalence: Mathematical meaning preserved across optimizations")

    print(f"\nüéØ Key Insights:")
    print("   ‚Ä¢ ML semantics can be preserved while optimizing for GPU kernels")
    print("   ‚Ä¢ Different architectures benefit from different optimization patterns")
    print("   ‚Ä¢ Understanding both ML concepts AND kernel efficiency is powerful")
    print("   ‚Ä¢ Progressive optimization allows gradual performance improvements")

    print(f"\n‚úÖ Semantic Models Demo Complete!")


if __name__ == "__main__":
    main()