name: AMD GPU Tests

on:
  # Weekly schedule â€” Mondays at 3 AM UTC
  schedule:
    - cron: '0 3 * * 1'
  # Manual trigger for on-demand testing
  workflow_dispatch:
    inputs:
      test_level:
        description: 'Validation level'
        required: false
        default: 'standard'
        type: choice
        options:
          - quick
          - standard
          - full
      instance_type:
        description: 'AWS instance type'
        required: false
        default: 'g5.xlarge'
        type: choice
        options:
          - g5.xlarge
          - p4d.24xlarge
  # Run on pushes that touch AMD backend code
  push:
    branches: [main]
    paths:
      - 'src/torchbridge/backends/amd/**'
      - 'tests/backends/test_amd_*.py'
      - 'docker/Dockerfile.amd'

concurrency:
  group: amd-gpu-${{ github.ref }}
  cancel-in-progress: true

env:
  BENCHMARK_RESULTS_DIR: benchmark-results-amd

jobs:
  # ---------------------------------------------------------------------------
  # AMD GPU Unit + Integration Tests
  # ---------------------------------------------------------------------------
  test-amd:
    name: AMD GPU Tests
    runs-on: [self-hosted, amd-gpu, linux]
    timeout-minutes: 60
    steps:
      - name: Checkout repository
        uses: actions/checkout@v6

      - name: Verify ROCm installation
        run: |
          echo "=== ROCm Info ==="
          rocm-smi || echo "rocm-smi not available"
          rocminfo | head -30 || echo "rocminfo not available"
          echo ""
          echo "=== PyTorch ROCm check ==="
          python3 -c "
          import torch
          print(f'PyTorch: {torch.__version__}')
          print(f'CUDA (HIP) available: {torch.cuda.is_available()}')
          print(f'HIP version: {torch.version.hip}')
          if torch.cuda.is_available():
              print(f'GPU count: {torch.cuda.device_count()}')
              for i in range(torch.cuda.device_count()):
                  print(f'  GPU {i}: {torch.cuda.get_device_name(i)}')
          "

      - name: Install dependencies
        run: |
          python3 -m pip install --upgrade pip
          pip install -e ".[dev]"

      - name: Run AMD-specific tests
        run: |
          PYTHONPATH=src pytest tests/ -v --tb=short \
            -m "amd" \
            --junitxml=test-results-amd.xml \
            || true

      - name: Run TorchBridge AMD validation
        run: |
          PYTHONPATH=src python3 -m torchbridge.cli.validate \
            --ci \
            --level ${{ github.event.inputs.test_level || 'standard' }} \
            --output amd-validation-report.json \
            --format json
        continue-on-error: true

      - name: Run TorchBridge doctor (CI mode)
        run: |
          PYTHONPATH=src python3 -m torchbridge.cli.doctor --ci \
            > doctor-amd-report.json 2>&1
        continue-on-error: true

      - name: Upload test results
        uses: actions/upload-artifact@v6
        if: always()
        with:
          name: amd-gpu-test-results
          path: |
            test-results-amd.xml
            amd-validation-report.json
            doctor-amd-report.json
          retention-days: 30

  # ---------------------------------------------------------------------------
  # AMD GPU Benchmarks
  # ---------------------------------------------------------------------------
  benchmark-amd:
    name: AMD GPU Benchmarks
    runs-on: [self-hosted, amd-gpu, linux]
    needs: test-amd
    timeout-minutes: 30
    steps:
      - name: Checkout repository
        uses: actions/checkout@v6

      - name: Install dependencies
        run: |
          python3 -m pip install --upgrade pip
          pip install -e ".[dev]"

      - name: Run AMD benchmarks
        run: |
          mkdir -p ${{ env.BENCHMARK_RESULTS_DIR }}
          PYTHONPATH=src python3 -m torchbridge.cli.benchmark \
            --predefined optimization --quick \
            --format json \
            --output ${{ env.BENCHMARK_RESULTS_DIR }}/benchmark-amd.json \
            || true

      - name: Run AMD benchmarks (CSV)
        run: |
          PYTHONPATH=src python3 -m torchbridge.cli.benchmark \
            --predefined optimization --quick \
            --format csv \
            --output ${{ env.BENCHMARK_RESULTS_DIR }}/benchmark-amd.csv \
            || true

      - name: Upload benchmark results
        uses: actions/upload-artifact@v6
        if: always()
        with:
          name: benchmark-results-amd
          path: ${{ env.BENCHMARK_RESULTS_DIR }}/
          retention-days: 90

  # ---------------------------------------------------------------------------
  # Docker Build Verification
  # ---------------------------------------------------------------------------
  docker-build-amd:
    name: Build AMD Docker Image
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v6

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Build AMD image (no push)
        uses: docker/build-push-action@v6
        with:
          context: .
          file: ./docker/Dockerfile.amd
          push: false
          tags: torchbridge:amd-test
          cache-from: type=gha
          cache-to: type=gha,mode=max

  # ---------------------------------------------------------------------------
  # Report Summary
  # ---------------------------------------------------------------------------
  report:
    name: AMD Test Report
    runs-on: ubuntu-latest
    needs: [test-amd, benchmark-amd, docker-build-amd]
    if: always()
    steps:
      - name: Download test results
        uses: actions/download-artifact@v7
        with:
          name: amd-gpu-test-results
          path: results/
        continue-on-error: true

      - name: Download benchmark results
        uses: actions/download-artifact@v7
        with:
          name: benchmark-results-amd
          path: benchmarks/
        continue-on-error: true

      - name: Generate summary
        run: |
          echo "## AMD GPU Test Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Job | Status |" >> $GITHUB_STEP_SUMMARY
          echo "|-----|--------|" >> $GITHUB_STEP_SUMMARY
          echo "| GPU Tests | ${{ needs.test-amd.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Benchmarks | ${{ needs.benchmark-amd.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Docker Build | ${{ needs.docker-build-amd.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Artifacts" >> $GITHUB_STEP_SUMMARY
          echo "- Test results: \`amd-gpu-test-results\`" >> $GITHUB_STEP_SUMMARY
          echo "- Benchmark results: \`benchmark-results-amd\`" >> $GITHUB_STEP_SUMMARY

          if [ -f results/amd-validation-report.json ]; then
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "### Validation Report" >> $GITHUB_STEP_SUMMARY
            echo '```json' >> $GITHUB_STEP_SUMMARY
            cat results/amd-validation-report.json >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
          fi
