name: Intel GPU Tests

on:
  # Weekly schedule â€” Tuesdays at 3 AM UTC
  schedule:
    - cron: '0 3 * * 2'
  # Manual trigger for on-demand testing
  workflow_dispatch:
    inputs:
      test_level:
        description: 'Validation level'
        required: false
        default: 'standard'
        type: choice
        options:
          - quick
          - standard
          - full
      platform:
        description: 'Intel platform'
        required: false
        default: 'devcloud'
        type: choice
        options:
          - devcloud
          - max-series
          - arc
  # Run on pushes that touch Intel backend code
  push:
    branches: [main]
    paths:
      - 'src/torchbridge/backends/intel/**'
      - 'tests/backends/test_intel_*.py'
      - 'docker/Dockerfile.intel'

concurrency:
  group: intel-gpu-${{ github.ref }}
  cancel-in-progress: true

env:
  BENCHMARK_RESULTS_DIR: benchmark-results-intel

jobs:
  # ---------------------------------------------------------------------------
  # Intel GPU Unit + Integration Tests
  # ---------------------------------------------------------------------------
  test-intel:
    name: Intel GPU Tests
    runs-on: [self-hosted, intel-gpu, linux]
    timeout-minutes: 60
    steps:
      - name: Checkout repository
        uses: actions/checkout@v6

      - name: Verify Intel GPU / IPEX installation
        run: |
          echo "=== Intel GPU Info ==="
          sycl-ls 2>/dev/null || echo "sycl-ls not available"
          xpu-smi discovery 2>/dev/null || echo "xpu-smi not available"
          echo ""
          echo "=== oneAPI environment ==="
          echo "ONEAPI_ROOT: ${ONEAPI_ROOT:-not set}"
          echo ""
          echo "=== PyTorch IPEX check ==="
          python3 -c "
          import torch
          print(f'PyTorch: {torch.__version__}')
          try:
              import intel_extension_for_pytorch as ipex
              print(f'IPEX: {ipex.__version__}')
              print(f'XPU available: {torch.xpu.is_available()}')
              if torch.xpu.is_available():
                  print(f'XPU device count: {torch.xpu.device_count()}')
                  for i in range(torch.xpu.device_count()):
                      print(f'  XPU {i}: {torch.xpu.get_device_name(i)}')
          except ImportError:
              print('IPEX not installed')
          except Exception as e:
              print(f'IPEX check error: {e}')
          "

      - name: Install dependencies
        run: |
          python3 -m pip install --upgrade pip
          pip install -e ".[dev]"

      - name: Run Intel-specific tests
        run: |
          PYTHONPATH=src pytest tests/ -v --tb=short \
            -m "intel" \
            --junitxml=test-results-intel.xml \
            || true

      - name: Run TorchBridge Intel validation
        run: |
          PYTHONPATH=src python3 -m torchbridge.cli.validate \
            --ci \
            --level ${{ github.event.inputs.test_level || 'standard' }} \
            --output intel-validation-report.json \
            --format json
        continue-on-error: true

      - name: Run TorchBridge doctor (CI mode)
        run: |
          PYTHONPATH=src python3 -m torchbridge.cli.doctor --ci \
            > doctor-intel-report.json 2>&1
        continue-on-error: true

      - name: Upload test results
        uses: actions/upload-artifact@v6
        if: always()
        with:
          name: intel-gpu-test-results
          path: |
            test-results-intel.xml
            intel-validation-report.json
            doctor-intel-report.json
          retention-days: 30

  # ---------------------------------------------------------------------------
  # Intel GPU Benchmarks
  # ---------------------------------------------------------------------------
  benchmark-intel:
    name: Intel GPU Benchmarks
    runs-on: [self-hosted, intel-gpu, linux]
    needs: test-intel
    timeout-minutes: 30
    steps:
      - name: Checkout repository
        uses: actions/checkout@v6

      - name: Install dependencies
        run: |
          python3 -m pip install --upgrade pip
          pip install -e ".[dev]"

      - name: Run Intel benchmarks
        run: |
          mkdir -p ${{ env.BENCHMARK_RESULTS_DIR }}
          PYTHONPATH=src python3 -m torchbridge.cli.benchmark \
            --predefined optimization --quick \
            --format json \
            --output ${{ env.BENCHMARK_RESULTS_DIR }}/benchmark-intel.json \
            || true

      - name: Run Intel benchmarks (CSV)
        run: |
          PYTHONPATH=src python3 -m torchbridge.cli.benchmark \
            --predefined optimization --quick \
            --format csv \
            --output ${{ env.BENCHMARK_RESULTS_DIR }}/benchmark-intel.csv \
            || true

      - name: Upload benchmark results
        uses: actions/upload-artifact@v6
        if: always()
        with:
          name: benchmark-results-intel
          path: ${{ env.BENCHMARK_RESULTS_DIR }}/
          retention-days: 90

  # ---------------------------------------------------------------------------
  # Docker Build Verification
  # ---------------------------------------------------------------------------
  docker-build-intel:
    name: Build Intel Docker Image
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v6

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Build Intel image (no push)
        uses: docker/build-push-action@v6
        with:
          context: .
          file: ./docker/Dockerfile.intel
          push: false
          tags: torchbridge:intel-test
          cache-from: type=gha
          cache-to: type=gha,mode=max

  # ---------------------------------------------------------------------------
  # Report Summary
  # ---------------------------------------------------------------------------
  report:
    name: Intel Test Report
    runs-on: ubuntu-latest
    needs: [test-intel, benchmark-intel, docker-build-intel]
    if: always()
    steps:
      - name: Download test results
        uses: actions/download-artifact@v7
        with:
          name: intel-gpu-test-results
          path: results/
        continue-on-error: true

      - name: Download benchmark results
        uses: actions/download-artifact@v7
        with:
          name: benchmark-results-intel
          path: benchmarks/
        continue-on-error: true

      - name: Generate summary
        run: |
          echo "## Intel GPU Test Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Job | Status |" >> $GITHUB_STEP_SUMMARY
          echo "|-----|--------|" >> $GITHUB_STEP_SUMMARY
          echo "| GPU Tests | ${{ needs.test-intel.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Benchmarks | ${{ needs.benchmark-intel.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Docker Build | ${{ needs.docker-build-intel.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Artifacts" >> $GITHUB_STEP_SUMMARY
          echo "- Test results: \`intel-gpu-test-results\`" >> $GITHUB_STEP_SUMMARY
          echo "- Benchmark results: \`benchmark-results-intel\`" >> $GITHUB_STEP_SUMMARY

          if [ -f results/intel-validation-report.json ]; then
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "### Validation Report" >> $GITHUB_STEP_SUMMARY
            echo '```json' >> $GITHUB_STEP_SUMMARY
            cat results/intel-validation-report.json >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
          fi
