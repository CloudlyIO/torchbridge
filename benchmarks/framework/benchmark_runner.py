#!/usr/bin/env python3
"""
Advanced Benchmark Runner for PyTorch Optimization Framework

Comprehensive benchmarking against state-of-the-art implementations with
statistical analysis and production-grade measurement methodology.
"""

import sys
import os
import time
import torch
import numpy as np
from typing import Dict, List, Any, Optional, Callable, Tuple
from dataclasses import dataclass, field
from enum import Enum
import json
from datetime import datetime
import psutil

# Add src to path for our optimizations
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', '..', 'src'))

class BenchmarkType(Enum):
    INFERENCE = "inference"
    TRAINING = "training"
    MEMORY = "memory"
    ACCURACY = "accuracy"
    SCALING = "scaling"

@dataclass
class BenchmarkConfig:
    """Configuration for benchmark execution"""
    name: str
    benchmark_type: BenchmarkType
    model_config: Dict[str, Any]
    batch_sizes: List[int] = field(default_factory=lambda: [1, 4, 8, 16, 32])
    sequence_lengths: List[int] = field(default_factory=lambda: [128, 512, 1024, 2048])
    num_trials: int = 100
    warmup_trials: int = 10
    device: str = "auto"
    precision: str = "float32"
    enable_compilation: bool = True
    enable_profiling: bool = False

@dataclass
class PerformanceMetrics:
    """Performance measurement results"""
    latency_ms: float
    throughput_samples_per_sec: float
    peak_memory_mb: float
    memory_efficiency: float
    accuracy_loss: float
    statistical_significance: bool
    confidence_interval_95: Tuple[float, float]

class BaseImplementation:
    """Base class for benchmark implementations"""

    def __init__(self, name: str, device: torch.device):
        self.name = name
        self.device = device

    def setup_model(self, model_config: Dict[str, Any]) -> torch.nn.Module:
        """Setup the model for benchmarking"""
        raise NotImplementedError

    def run_inference(self, model: torch.nn.Module, inputs: torch.Tensor) -> torch.Tensor:
        """Run inference with the model"""
        raise NotImplementedError

    def run_training_step(self, model: torch.nn.Module, inputs: torch.Tensor, targets: torch.Tensor) -> float:
        """Run a training step"""
        raise NotImplementedError

class BenchmarkRunner:
    """
    Production-grade benchmark runner for optimization comparison.

    Features:
    - Statistical significance testing
    - Multiple baseline comparison
    - Hardware-aware optimization
    - Comprehensive metrics collection
    """

    def __init__(self, output_dir: str = "benchmarks/results"):
        self.output_dir = output_dir
        self.baselines = {}
        self.device = self._detect_optimal_device()
        self.results = {}

        # Create output directory
        os.makedirs(output_dir, exist_ok=True)

        print(f"ðŸ Benchmark Runner initialized")
        print(f"   Device: {self.device}")
        print(f"   Output: {output_dir}")

    def _detect_optimal_device(self) -> torch.device:
        """Detect the best available device for benchmarking"""
        if torch.cuda.is_available():
            device = torch.device("cuda:0")
            print(f"   GPU: {torch.cuda.get_device_name()}")
            print(f"   Memory: {torch.cuda.get_device_properties(0).total_memory // 1024**3}GB")
        else:
            device = torch.device("cpu")
            print(f"   CPU: {psutil.cpu_count()} cores")

        return device

    def register_baseline(self, baseline: BaseImplementation):
        """Register a baseline implementation for comparison"""
        self.baselines[baseline.name] = baseline
        print(f"   âœ… Registered baseline: {baseline.name}")

    def register_optimized_implementation(self, name: str, implementation: BaseImplementation):
        """Register our optimized implementation"""
        self.baselines[name] = implementation
        print(f"   ðŸš€ Registered optimization: {name}")

    def run_comprehensive_benchmark(self, config: BenchmarkConfig) -> Dict[str, PerformanceMetrics]:
        """Run comprehensive benchmark across all registered implementations"""

        print(f"\nðŸ Running Comprehensive Benchmark: {config.name}")
        print(f"   Type: {config.benchmark_type.value}")
        print(f"   Trials: {config.num_trials} (+ {config.warmup_trials} warmup)")
        print()

        benchmark_results = {}

        # Test each implementation
        for impl_name, implementation in self.baselines.items():
            print(f"   ðŸ”§ Benchmarking {impl_name}...")

            try:
                if config.benchmark_type == BenchmarkType.INFERENCE:
                    metrics = self._benchmark_inference(implementation, config)
                elif config.benchmark_type == BenchmarkType.TRAINING:
                    metrics = self._benchmark_training(implementation, config)
                elif config.benchmark_type == BenchmarkType.MEMORY:
                    metrics = self._benchmark_memory(implementation, config)
                else:
                    metrics = self._benchmark_scaling(implementation, config)

                benchmark_results[impl_name] = metrics

                print(f"      âœ… {impl_name}: {metrics.latency_ms:.2f}ms, {metrics.throughput_samples_per_sec:.1f}/s")

            except Exception as e:
                print(f"      âŒ {impl_name}: Benchmark failed - {e}")
                benchmark_results[impl_name] = None

        # Statistical analysis
        print(f"\n   ðŸ“Š Statistical Analysis:")
        analysis_results = self._perform_statistical_analysis(benchmark_results)

        # Save results
        self._save_benchmark_results(config, benchmark_results, analysis_results)

        return benchmark_results

    def _benchmark_inference(self, implementation: BaseImplementation, config: BenchmarkConfig) -> PerformanceMetrics:
        """Benchmark inference performance"""

        # Setup model
        model = implementation.setup_model(config.model_config)
        model.eval()

        # Test with different batch sizes and sequence lengths
        best_throughput = 0
        best_latency = float('inf')
        all_measurements = []

        for batch_size in config.batch_sizes[:2]:  # Test first 2 batch sizes for speed
            for seq_len in config.sequence_lengths[:2]:  # Test first 2 seq lengths

                # Create test input
                inputs = torch.randn(
                    batch_size, seq_len, config.model_config.get('hidden_size', 768),
                    device=self.device
                )

                # Warmup
                for _ in range(config.warmup_trials):
                    with torch.no_grad():
                        _ = implementation.run_inference(model, inputs)
                        if self.device.type == 'cuda':
                            torch.cuda.synchronize()

                # Memory tracking
                if self.device.type == 'cuda':
                    torch.cuda.reset_peak_memory_stats()

                # Benchmark
                times = []
                for _ in range(config.num_trials):
                    start_time = time.perf_counter()

                    with torch.no_grad():
                        output = implementation.run_inference(model, inputs)

                    if self.device.type == 'cuda':
                        torch.cuda.synchronize()

                    end_time = time.perf_counter()
                    times.append(end_time - start_time)

                # Calculate metrics
                avg_time = np.mean(times)
                std_time = np.std(times)
                throughput = batch_size / avg_time

                # Memory usage
                peak_memory = 0
                if self.device.type == 'cuda':
                    peak_memory = torch.cuda.max_memory_allocated() / 1024**2

                all_measurements.append({
                    'latency': avg_time,
                    'throughput': throughput,
                    'memory': peak_memory,
                    'batch_size': batch_size,
                    'seq_len': seq_len
                })

                if avg_time < best_latency:
                    best_latency = avg_time

                if throughput > best_throughput:
                    best_throughput = throughput

        # Calculate confidence interval
        all_latencies = [m['latency'] for m in all_measurements]
        mean_latency = np.mean(all_latencies)
        std_latency = np.std(all_latencies)
        n = len(all_latencies)
        confidence_interval = (
            mean_latency - 1.96 * std_latency / np.sqrt(n),
            mean_latency + 1.96 * std_latency / np.sqrt(n)
        )

        return PerformanceMetrics(
            latency_ms=best_latency * 1000,
            throughput_samples_per_sec=best_throughput,
            peak_memory_mb=max([m['memory'] for m in all_measurements]),
            memory_efficiency=1.0,  # To be calculated relative to baseline
            accuracy_loss=0.0,  # To be measured separately
            statistical_significance=True,  # To be calculated in analysis
            confidence_interval_95=confidence_interval
        )

    def _benchmark_training(self, implementation: BaseImplementation, config: BenchmarkConfig) -> PerformanceMetrics:
        """Benchmark training performance"""
        # Simplified training benchmark
        model = implementation.setup_model(config.model_config)
        model.train()

        batch_size = config.batch_sizes[1]  # Use medium batch size
        seq_len = config.sequence_lengths[1]  # Use medium sequence length

        inputs = torch.randn(batch_size, seq_len, config.model_config.get('hidden_size', 768), device=self.device)
        targets = torch.randint(0, config.model_config.get('vocab_size', 50257), (batch_size, seq_len), device=self.device)

        # Warmup
        for _ in range(config.warmup_trials):
            loss = implementation.run_training_step(model, inputs, targets)

        # Benchmark
        times = []
        for _ in range(min(config.num_trials, 20)):  # Fewer trials for training
            start_time = time.perf_counter()
            loss = implementation.run_training_step(model, inputs, targets)
            if self.device.type == 'cuda':
                torch.cuda.synchronize()
            times.append(time.perf_counter() - start_time)

        avg_time = np.mean(times)
        throughput = batch_size / avg_time

        return PerformanceMetrics(
            latency_ms=avg_time * 1000,
            throughput_samples_per_sec=throughput,
            peak_memory_mb=0.0,  # To be measured
            memory_efficiency=1.0,
            accuracy_loss=0.0,
            statistical_significance=True,
            confidence_interval_95=(avg_time * 0.95, avg_time * 1.05)
        )

    def _benchmark_memory(self, implementation: BaseImplementation, config: BenchmarkConfig) -> PerformanceMetrics:
        """Benchmark memory efficiency"""
        if self.device.type != 'cuda':
            # Return placeholder for CPU
            return PerformanceMetrics(
                latency_ms=0.0,
                throughput_samples_per_sec=0.0,
                peak_memory_mb=0.0,
                memory_efficiency=1.0,
                accuracy_loss=0.0,
                statistical_significance=False,
                confidence_interval_95=(0.0, 0.0)
            )

        model = implementation.setup_model(config.model_config)
        model.eval()

        memory_measurements = []

        for batch_size in config.batch_sizes:
            inputs = torch.randn(batch_size, 512, config.model_config.get('hidden_size', 768), device=self.device)

            torch.cuda.reset_peak_memory_stats()

            with torch.no_grad():
                _ = implementation.run_inference(model, inputs)

            peak_memory = torch.cuda.max_memory_allocated() / 1024**2
            memory_measurements.append(peak_memory)

        return PerformanceMetrics(
            latency_ms=0.0,
            throughput_samples_per_sec=0.0,
            peak_memory_mb=max(memory_measurements),
            memory_efficiency=1.0,
            accuracy_loss=0.0,
            statistical_significance=True,
            confidence_interval_95=(0.0, 0.0)
        )

    def _benchmark_scaling(self, implementation: BaseImplementation, config: BenchmarkConfig) -> PerformanceMetrics:
        """Benchmark scaling characteristics"""
        # Placeholder for scaling benchmark
        return self._benchmark_inference(implementation, config)

    def _perform_statistical_analysis(self, results: Dict[str, PerformanceMetrics]) -> Dict[str, Any]:
        """Perform statistical analysis on benchmark results"""

        valid_results = {k: v for k, v in results.items() if v is not None}

        if len(valid_results) < 2:
            return {"analysis": "Insufficient data for statistical analysis"}

        # Find baseline (usually PyTorch Native)
        baseline_name = None
        for name in ["PyTorch Native", "Baseline", "pytorch_native"]:
            if name in valid_results:
                baseline_name = name
                break

        if not baseline_name:
            baseline_name = list(valid_results.keys())[0]

        baseline_metrics = valid_results[baseline_name]

        analysis = {
            "baseline": baseline_name,
            "comparisons": {},
            "rankings": {}
        }

        # Compare each implementation to baseline
        for name, metrics in valid_results.items():
            if name == baseline_name:
                continue

            # Calculate speedup with division by zero protection
            speedup = baseline_metrics.latency_ms / metrics.latency_ms if metrics.latency_ms > 0 else 0.0

            # Calculate throughput improvement with division by zero protection
            throughput_improvement = ((metrics.throughput_samples_per_sec / baseline_metrics.throughput_samples_per_sec - 1) * 100
                                     if baseline_metrics.throughput_samples_per_sec > 0 else 0.0)

            # Calculate memory reduction with division by zero protection
            memory_reduction = ((baseline_metrics.peak_memory_mb - metrics.peak_memory_mb) / baseline_metrics.peak_memory_mb * 100
                               if baseline_metrics.peak_memory_mb > 0 else 0.0)

            analysis["comparisons"][name] = {
                "speedup": speedup,
                "throughput_improvement_pct": throughput_improvement,
                "memory_reduction_pct": memory_reduction,
                "significant": speedup > 1.2  # 20% improvement threshold
            }

        # Rank implementations by performance
        rankings = sorted(valid_results.items(), key=lambda x: x[1].latency_ms)
        analysis["rankings"] = {
            "by_latency": [name for name, _ in rankings],
            "by_throughput": sorted(valid_results.keys(), key=lambda x: valid_results[x].throughput_samples_per_sec, reverse=True)
        }

        return analysis

    def _save_benchmark_results(self, config: BenchmarkConfig, results: Dict[str, PerformanceMetrics], analysis: Dict[str, Any]):
        """Save benchmark results to file"""

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"{config.name}_{config.benchmark_type.value}_{timestamp}.json"
        filepath = os.path.join(self.output_dir, filename)

        # Convert results to serializable format
        serializable_results = {}
        for name, metrics in results.items():
            if metrics is not None:
                serializable_results[name] = {
                    "latency_ms": metrics.latency_ms,
                    "throughput_samples_per_sec": metrics.throughput_samples_per_sec,
                    "peak_memory_mb": metrics.peak_memory_mb,
                    "memory_efficiency": metrics.memory_efficiency,
                    "accuracy_loss": metrics.accuracy_loss,
                    "statistical_significance": metrics.statistical_significance,
                    "confidence_interval_95": metrics.confidence_interval_95
                }

        report_data = {
            "config": {
                "name": config.name,
                "benchmark_type": config.benchmark_type.value,
                "model_config": config.model_config,
                "num_trials": config.num_trials,
                "device": str(self.device)
            },
            "results": serializable_results,
            "analysis": analysis,
            "metadata": {
                "timestamp": timestamp,
                "framework_version": "1.0.0",
                "pytorch_version": torch.__version__,
                "cuda_version": torch.version.cuda if torch.cuda.is_available() else None
            }
        }

        with open(filepath, 'w') as f:
            json.dump(report_data, f, indent=2, default=str)

        print(f"   ðŸ’¾ Results saved: {filepath}")

        # Generate summary
        if analysis.get("comparisons"):
            print(f"\n   ðŸ“ˆ Performance Summary vs {analysis['baseline']}:")
            for name, comp in analysis["comparisons"].items():
                status = "ðŸš€ SIGNIFICANT" if comp["significant"] else "ðŸ“Š MEASURED"
                print(f"      {name}: {comp['speedup']:.2f}x speedup, {comp['throughput_improvement_pct']:+.1f}% throughput {status}")

def create_simple_gpt_config(hidden_size: int = 768, num_layers: int = 12, num_heads: int = 12) -> Dict[str, Any]:
    """Create a simple GPT-style model configuration"""
    return {
        "hidden_size": hidden_size,
        "num_layers": num_layers,
        "num_heads": num_heads,
        "vocab_size": 50257,
        "max_position_embeddings": 2048,
        "layer_norm_epsilon": 1e-5
    }