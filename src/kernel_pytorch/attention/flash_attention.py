"""
FlashAttention Implementations

Consolidated FlashAttention variants including v2, v3, and FP8 support.
All implementations are unified under a common interface while maintaining
the performance optimizations of each variant.
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Optional, Tuple, Union
from dataclasses import dataclass

from .core import BaseAttention, AttentionConfig, register_attention


@dataclass
class FP8AttentionConfig:
    """Configuration for FP8 attention optimizations"""
    use_fp8: bool = False
    async_compute: bool = True
    warp_specialization: bool = True
    fp8_recipe: Optional[str] = None


class FlashAttention2(BaseAttention):
    """
    FlashAttention v2 implementation.

    Provides memory-efficient attention computation with O(NÂ²) time complexity
    but O(N) memory complexity through recomputation and tiling.
    """

    def __init__(self, config: AttentionConfig):
        super().__init__(config)

        # FlashAttention specific parameters
        self.block_size = getattr(config, 'block_size', 128)
        self.num_splits = getattr(config, 'num_splits', 4)

    def _compute_attention(self,
                          q: torch.Tensor,
                          k: torch.Tensor,
                          v: torch.Tensor,
                          attention_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        FlashAttention v2 computation with memory-efficient tiling.
        """
        try:
            # Try to use native FlashAttention if available
            import flash_attn
            from flash_attn import flash_attn_func

            # Reshape for flash_attn: [B, S, H, D_h]
            batch_size, num_heads, seq_len, head_dim = q.shape

            q_fa = q.transpose(1, 2).contiguous()  # [B, S, H, D_h]
            k_fa = k.transpose(1, 2).contiguous()
            v_fa = v.transpose(1, 2).contiguous()

            # FlashAttention call
            output = flash_attn_func(
                q_fa, k_fa, v_fa,
                dropout_p=self.config.attention_dropout if self.training else 0.0,
                causal=self.config.causal,
                softmax_scale=self.scaling
            )

            # Reshape back to [B, H, S, D_h]
            output = output.transpose(1, 2)

            return output

        except ImportError:
            # Fallback to memory-efficient implementation
            return self._memory_efficient_attention(q, k, v, attention_mask)

    def _memory_efficient_attention(self,
                                   q: torch.Tensor,
                                   k: torch.Tensor,
                                   v: torch.Tensor,
                                   attention_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        Memory-efficient attention fallback implementation.
        """
        batch_size, num_heads, seq_len, head_dim = q.shape

        # Use PyTorch's memory-efficient attention if available
        try:
            # PyTorch 2.0+ has memory-efficient attention
            output = F.scaled_dot_product_attention(
                q, k, v,
                attn_mask=attention_mask,
                dropout_p=self.config.attention_dropout if self.training else 0.0,
                is_causal=self.config.causal
            )
            return output
        except AttributeError:
            # Fallback to standard attention with chunking
            return self._chunked_attention(q, k, v, attention_mask)

    def _chunked_attention(self,
                          q: torch.Tensor,
                          k: torch.Tensor,
                          v: torch.Tensor,
                          attention_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        Chunked attention computation to reduce memory usage.
        """
        batch_size, num_heads, seq_len, head_dim = q.shape
        chunk_size = min(self.block_size, seq_len)

        output = torch.zeros_like(q)

        for i in range(0, seq_len, chunk_size):
            end_i = min(i + chunk_size, seq_len)
            q_chunk = q[:, :, i:end_i]

            # Compute attention scores for this chunk
            attn_scores = torch.matmul(q_chunk, k.transpose(-2, -1)) * self.scaling

            if attention_mask is not None:
                mask_chunk = attention_mask[:, :, i:end_i]
                attn_scores = attn_scores.masked_fill(mask_chunk == 0, -1e9)

            if self.config.causal:
                causal_mask = torch.tril(torch.ones(end_i - i, seq_len, device=q.device, dtype=torch.bool))
                attn_scores = attn_scores.masked_fill(~causal_mask, -1e9)

            attn_weights = F.softmax(attn_scores, dim=-1)
            attn_weights = self.attention_dropout(attn_weights)

            output[:, :, i:end_i] = torch.matmul(attn_weights, v)

        return output


class FlashAttention3(BaseAttention):
    """
    FlashAttention v3 implementation with FP8 support.

    Provides the latest optimizations including:
    - FP8 precision support
    - Asynchronous computation
    - Warp-level optimizations
    - Improved memory access patterns
    """

    def __init__(self, config: AttentionConfig, fp8_config: Optional[FP8AttentionConfig] = None):
        super().__init__(config)

        self.fp8_config = fp8_config or FP8AttentionConfig()
        self.use_fp8 = config.use_fp8 or self.fp8_config.use_fp8

        # FlashAttention v3 specific optimizations
        self.async_compute = self.fp8_config.async_compute
        self.warp_specialization = self.fp8_config.warp_specialization

    def _compute_attention(self,
                          q: torch.Tensor,
                          k: torch.Tensor,
                          v: torch.Tensor,
                          attention_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        FlashAttention v3 computation with FP8 optimizations.
        """
        if self.use_fp8:
            return self._fp8_attention(q, k, v, attention_mask)
        else:
            # Use FlashAttention v2 as fallback
            flash_v2 = FlashAttention2(self.config)
            return flash_v2._compute_attention(q, k, v, attention_mask)

    def _fp8_attention(self,
                      q: torch.Tensor,
                      k: torch.Tensor,
                      v: torch.Tensor,
                      attention_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        FP8 optimized attention computation.
        """
        # For now, simulate FP8 by using reduced precision
        # In a real implementation, this would use specialized FP8 kernels

        original_dtype = q.dtype

        try:
            # Simulate FP8 computation with bfloat16
            if original_dtype == torch.float32:
                q_fp8 = q.to(torch.bfloat16)
                k_fp8 = k.to(torch.bfloat16)
                v_fp8 = v.to(torch.bfloat16)

                # Compute attention in reduced precision
                attn_scores = torch.matmul(q_fp8, k_fp8.transpose(-2, -1)) * self.scaling

                # Apply masks and softmax
                if attention_mask is not None:
                    attn_scores = attn_scores.masked_fill(attention_mask == 0, -1e4)  # Reduced range for FP8

                if self.config.causal:
                    seq_len = q.size(-2)
                    causal_mask = torch.tril(torch.ones(seq_len, seq_len, device=q.device, dtype=torch.bool))
                    attn_scores = attn_scores.masked_fill(~causal_mask, -1e4)

                attn_weights = F.softmax(attn_scores, dim=-1)
                attn_weights = self.attention_dropout(attn_weights)

                output = torch.matmul(attn_weights, v_fp8)

                # Convert back to original precision
                return output.to(original_dtype)
            else:
                # For non-float32 inputs, use standard computation
                return FlashAttention2(self.config)._compute_attention(q, k, v, attention_mask)

        except Exception:
            # Fallback to standard attention
            return FlashAttention2(self.config)._compute_attention(q, k, v, attention_mask)


class MultiHeadFlashAttention(nn.Module):
    """
    Multi-head wrapper for FlashAttention with simplified interface.
    """

    def __init__(self,
                 embed_dim: int,
                 num_heads: int,
                 version: str = "v3",
                 dropout: float = 0.0,
                 causal: bool = False,
                 use_fp8: bool = False):
        super().__init__()

        config = AttentionConfig(
            embed_dim=embed_dim,
            num_heads=num_heads,
            dropout=dropout,
            causal=causal,
            use_fp8=use_fp8,
            pattern=AttentionConfig.AttentionPatterns.CAUSAL if causal else AttentionConfig.AttentionPatterns.FULL
        )

        if version == "v3":
            self.attention = FlashAttention3(config)
        elif version == "v2":
            self.attention = FlashAttention2(config)
        else:
            raise ValueError(f"Unsupported FlashAttention version: {version}")

    def forward(self,
                x: torch.Tensor,
                attention_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        """Forward pass for self-attention"""
        return self.attention(x, attention_mask=attention_mask)


# Register FlashAttention implementations
register_attention('flash_attention2', FlashAttention2)
register_attention('flash_attention3', FlashAttention3)
register_attention('flash_attention', FlashAttention3)  # Default to v3


# Factory functions for easy creation
def create_flash_attention(embed_dim: int,
                         num_heads: int,
                         version: str = "v3",
                         **kwargs) -> BaseAttention:
    """Create a FlashAttention module with specified version"""
    config = AttentionConfig(embed_dim=embed_dim, num_heads=num_heads, **kwargs)

    if version == "v3":
        return FlashAttention3(config)
    elif version == "v2":
        return FlashAttention2(config)
    else:
        raise ValueError(f"Unsupported version: {version}")


def create_fp8_attention(embed_dim: int,
                        num_heads: int,
                        fp8_config: Optional[FP8AttentionConfig] = None,
                        **kwargs) -> FlashAttention3:
    """Create FlashAttention v3 with FP8 optimizations"""
    config = AttentionConfig(embed_dim=embed_dim, num_heads=num_heads, use_fp8=True, **kwargs)
    return FlashAttention3(config, fp8_config)