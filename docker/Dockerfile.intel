# TorchBridge Intel GPU Container
# Intel Extension for PyTorch (IPEX) container for Intel GPU/XPU inference and training
#
# Build:
#   docker build -f docker/Dockerfile.intel -t torchbridge:intel .
#
# Run (with Intel GPU passthrough):
#   docker run --device=/dev/dri --group-add render \
#     -p 8000:8000 torchbridge:intel
#
# Tested on: Intel Data Center GPU Max (Ponte Vecchio), Arc A-series
# Version: 0.4.42

# Intel IPEX base image with PyTorch and oneAPI
FROM intel/intel-extension-for-pytorch:2.1.30-xpu AS base

# Set environment variables
ENV PYTHONDONTWRITEBYTECODE=1 \
    PYTHONUNBUFFERED=1 \
    PYTHONIOENCODING=UTF-8 \
    LANG=C.UTF-8 \
    LC_ALL=C.UTF-8 \
    PIP_NO_CACHE_DIR=1 \
    PIP_DISABLE_PIP_VERSION_CHECK=1

# oneAPI environment (already set by base image, reinforced here)
ENV ONEAPI_ROOT=/opt/intel/oneapi \
    DPCPPROOT=/opt/intel/oneapi/compiler/latest \
    MKLROOT=/opt/intel/oneapi/mkl/latest

# Install additional system dependencies
RUN apt-get update && apt-get install -y --no-install-recommends \
    git \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Create app directory
WORKDIR /app

# Install Python dependencies
COPY requirements.txt ./
RUN pip install --upgrade pip && \
    pip install -r requirements.txt

# Install optional serving dependencies
RUN pip install fastapi uvicorn prometheus-client psutil

# Copy application code
COPY src/ ./src/
COPY pyproject.toml ./

# Install TorchBridge
RUN pip install -e .

# Verify IPEX + PyTorch integration
RUN python -c "\
import torch; \
print(f'PyTorch {torch.__version__}'); \
try: \
    import intel_extension_for_pytorch as ipex; \
    print(f'IPEX {ipex.__version__}'); \
    print(f'XPU available: {torch.xpu.is_available()}'); \
    print(f'XPU device count: {torch.xpu.device_count()}'); \
except Exception as e: \
    print(f'IPEX check: {e}') \
" || true

# Create non-root user
RUN useradd -m -u 1000 appuser && \
    chown -R appuser:appuser /app
USER appuser

# Expose ports
# 8000: FastAPI inference server
# 9090: Prometheus metrics
EXPOSE 8000 9090

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:8000/health/live || exit 1

# Default command: Run FastAPI server
CMD ["python", "-m", "uvicorn", "torchbridge.deployment.serving.fastapi_server:app", "--host", "0.0.0.0", "--port", "8000"]

# Labels
LABEL org.opencontainers.image.title="TorchBridge Intel" \
      org.opencontainers.image.description="Production inference container with Intel XPU/GPU support via IPEX" \
      org.opencontainers.image.version="0.4.42" \
      org.opencontainers.image.vendor="TorchBridge" \
      org.opencontainers.image.source="https://github.com/your-org/torchbridge"
