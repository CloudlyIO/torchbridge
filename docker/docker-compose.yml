# TorchBridge Docker Compose Configuration
#
# Deploy complete inference stack with monitoring
#
# Usage:
#   docker-compose -f docker/docker-compose.yml up -d
#
# Services:
#   - inference: TorchBridge inference server
#   - prometheus: Metrics collection
#   - grafana: Metrics visualization
#
# Version: 0.3.10

version: '3.8'

services:
  # =============================================================================
  # TorchBridge Inference Server
  # =============================================================================
  inference:
    build:
      context: ..
      dockerfile: docker/Dockerfile.nvidia
    image: torchbridge:nvidia
    container_name: torchbridge-inference
    ports:
      - "8000:8000"
      - "9090:9090"
    environment:
      - MODEL_NAME=${MODEL_NAME:-model}
      - MODEL_PATH=${MODEL_PATH:-/models/model.pt}
      - DEVICE=${DEVICE:-cuda}
      - ENABLE_FP16=${ENABLE_FP16:-true}
      - LOG_LEVEL=${LOG_LEVEL:-info}
    volumes:
      - ${MODEL_DIR:-./models}:/models:ro
      - inference-logs:/app/logs
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health/live"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    restart: unless-stopped
    networks:
      - torchbridge-net

  # =============================================================================
  # Prometheus - Metrics Collection
  # =============================================================================
  prometheus:
    image: prom/prometheus:v2.48.0
    container_name: torchbridge-prometheus
    ports:
      - "9091:9090"
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus-data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=15d'
      - '--web.enable-lifecycle'
    restart: unless-stopped
    networks:
      - torchbridge-net
    depends_on:
      - inference

  # =============================================================================
  # Grafana - Metrics Visualization
  # =============================================================================
  grafana:
    image: grafana/grafana:10.2.2
    container_name: torchbridge-grafana
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_USER=${GRAFANA_USER:-admin}
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_PASSWORD:-admin}
      - GF_USERS_ALLOW_SIGN_UP=false
    volumes:
      - grafana-data:/var/lib/grafana
      - ./grafana/provisioning:/etc/grafana/provisioning:ro
    restart: unless-stopped
    networks:
      - torchbridge-net
    depends_on:
      - prometheus

  # =============================================================================
  # Inference AMD GPU
  # =============================================================================
  inference-amd:
    build:
      context: ..
      dockerfile: docker/Dockerfile.amd
    image: torchbridge:amd
    container_name: torchbridge-inference-amd
    profiles:
      - amd
    ports:
      - "8002:8000"
      - "9092:9090"
    environment:
      - MODEL_NAME=${MODEL_NAME:-model}
      - MODEL_PATH=${MODEL_PATH:-/models/model.pt}
      - DEVICE=${DEVICE:-cuda}
      - LOG_LEVEL=${LOG_LEVEL:-info}
    volumes:
      - ${MODEL_DIR:-./models}:/models:ro
    devices:
      - /dev/kfd:/dev/kfd
      - /dev/dri:/dev/dri
    group_add:
      - video
    restart: unless-stopped
    networks:
      - torchbridge-net

  # =============================================================================
  # Inference Intel GPU
  # =============================================================================
  inference-intel:
    build:
      context: ..
      dockerfile: docker/Dockerfile.intel
    image: torchbridge:intel
    container_name: torchbridge-inference-intel
    profiles:
      - intel
    ports:
      - "8003:8000"
      - "9093:9090"
    environment:
      - MODEL_NAME=${MODEL_NAME:-model}
      - MODEL_PATH=${MODEL_PATH:-/models/model.pt}
      - DEVICE=${DEVICE:-xpu}
      - LOG_LEVEL=${LOG_LEVEL:-info}
    volumes:
      - ${MODEL_DIR:-./models}:/models:ro
    devices:
      - /dev/dri:/dev/dri
    group_add:
      - render
    restart: unless-stopped
    networks:
      - torchbridge-net

  # =============================================================================
  # Inference CPU (Alternative)
  # =============================================================================
  inference-cpu:
    build:
      context: ..
      dockerfile: docker/Dockerfile.cpu
    image: torchbridge:cpu
    container_name: torchbridge-inference-cpu
    profiles:
      - cpu-only
    ports:
      - "8001:8000"
    environment:
      - MODEL_NAME=${MODEL_NAME:-model}
      - DEVICE=cpu
    volumes:
      - ${MODEL_DIR:-./models}:/models:ro
    restart: unless-stopped
    networks:
      - torchbridge-net

volumes:
  inference-logs:
  prometheus-data:
  grafana-data:

networks:
  torchbridge-net:
    driver: bridge
